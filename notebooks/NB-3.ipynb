{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Calling with LLM APIs\n",
    "\n",
    "## What This Covers\n",
    "\n",
    "Function calling (also called \"tool use\") lets LLMs interact with external systems: APIs, databases, calculators, or any Python function you define. Instead of just generating text, the model can request function calls with specific parameters, you execute them, and return results.\n",
    "\n",
    "This notebook shows:\n",
    "- **OpenAI function calling**: Weather API example with explicit execution flow\n",
    "- **Gemini function calling**: Same pattern using Google's API\n",
    "- **Key pattern**: Model decides → You execute → Model generates final response\n",
    "\n",
    "**The Core Idea:** You describe functions to the model. When appropriate, the model returns structured JSON saying \"call this function with these arguments.\" You execute it and feed results back. The model then uses that information to respond to the user.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- Define function schemas for OpenAI and Gemini\n",
    "- Handle the full execution cycle (request → execute → respond)\n",
    "- Understand when the model calls functions vs. responding directly\n",
    "- Work with real APIs (weather data example)\n",
    "\n",
    "**What's Next:** We'll explore LLM workflows where multiple LLMs coordinate.\n",
    "\n",
    "**By the end:** You'll understand how to connect LLMs to external systems and handle the execution flow for both OpenAI and Gemini."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Function Calling\n",
    "\n",
    "We'll start with OpenAI's function calling. The pattern:\n",
    "1. Define a function (e.g., `get_weather`)\n",
    "2. Describe it to the model using a schema\n",
    "3. Call the model with the function available\n",
    "4. Execute the function with the model's provided arguments\n",
    "5. Send results back for the final response\n",
    "\n",
    "Let's see it in action with a weather API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: Define the function.** We'll create `get_weather` that fetches temperature data from a free weather API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "openai_client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "def get_weather(latitude, longitude):\n",
    "    \"\"\"Fetch current temperature from Open-Meteo API.\"\"\"\n",
    "    response = requests.get(\n",
    "        f\"https://api.open-meteo.com/v1/forecast?\"\n",
    "        f\"latitude={latitude}&longitude={longitude}&current=temperature_2m\"\n",
    "    )\n",
    "    data = response.json()\n",
    "    return data['current']['temperature_2m']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Describe the function to the model.** We create a schema that tells OpenAI what the function does and what parameters it expects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "\n",
    "tools = [{\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"get_weather\",\n",
    "        \"description\": \"Get current temperature for provided coordinates in celsius.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"latitude\": {\"type\": \"number\"},\n",
    "                \"longitude\": {\"type\": \"number\"}\n",
    "            },\n",
    "            \"required\": [\"latitude\", \"longitude\"],\n",
    "            \"additionalProperties\": False\n",
    "        },\n",
    "        \"strict\": True\n",
    "    }\n",
    "}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: Call the model.** Send a query to OpenAI with the function schema available. The model \"decides\" whether to call the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatCompletionMessageFunctionToolCall(id='call_HmlGdZuQs5i5mhqtbZJQdNLy', function=Function(arguments='{\"latitude\":-33.8688,\"longitude\":151.2093}', name='get_weather'), type='function')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What's the weather like in Sydney today?\"\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": query }]\n",
    "\n",
    "# Pass tools parameter so model can choose to call get_weather\n",
    "completion = openai_client.chat.completions.create(\n",
    "    model=\"gpt-5-mini\",\n",
    "    messages=messages,\n",
    "    tools=tools,\n",
    ")\n",
    "\n",
    "completion.choices[0].message.tool_calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4: Execute the function.** Extract the function call from the model's response and run it with the provided arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the function name and arguments from model's response\n",
    "tool_call = completion.choices[0].message.tool_calls[0]\n",
    "args = json.loads(tool_call.function.arguments)\n",
    "\n",
    "# Execute the actual function\n",
    "result = get_weather(args[\"latitude\"], args[\"longitude\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5: Send results back.** Add the function result to the conversation and call the model again for the final user-facing response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add both the model's tool call and the result to conversation history\n",
    "messages.append(completion.choices[0].message)  \n",
    "messages.append({\n",
    "    \"role\": \"tool\",\n",
    "    \"tool_call_id\": tool_call.id,\n",
    "    \"content\": str(result)\n",
    "})\n",
    "\n",
    "# Call model again with the function result available\n",
    "completion_2 = openai_client.chat.completions.create(\n",
    "    model=\"gpt-5-mini\",\n",
    "    messages=messages,\n",
    "    tools=tools,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Current temperature in Sydney (CBD) is about 25.5°C (≈78.0°F) right now — pleasantly warm. \\n\\nWould you like a full forecast (hourly/daily), precipitation chance, wind/humidity, or clothing suggestions?'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion_2.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gemini Function Calling\n",
    "\n",
    "The same pattern works with Gemini's API, just with different syntax:\n",
    "1. Define and configure the function as a tool\n",
    "2. Call the model with the tool available\n",
    "3. Execute the function and send results back for the final response\n",
    "\n",
    "We'll use the same `get_weather` function and query defined above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import Gemini and create the client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "gem_client = genai.Client(api_key=os.getenv(\"GOOGLE_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: Define and configure the tool.** Create the function schema and wrap it as a Gemini tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the same get_weather function defined above\n",
    "get_weather_tool = {\n",
    "    \"name\": \"get_weather\",\n",
    "    \"description\": \"Get current temperature for provided coordinates in celsius. When the user asks about weather in a city you should determine the latitude and longitude coordinates for that city.\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"latitude\": {\"type\": \"number\"},\n",
    "            \"longitude\": {\"type\": \"number\"}\n",
    "        },\n",
    "        \"required\": [\"latitude\", \"longitude\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "tools = types.Tool(function_declarations=[get_weather_tool])\n",
    "config = types.GenerateContentConfig(tools=[tools])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Call the model.** Send the query with tools available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id=None args={'latitude': -33.8688, 'longitude': 151.2093} name='get_weather' partial_args=None will_continue=None\n"
     ]
    }
   ],
   "source": [
    "query = \"What's the weather like in Sydney today?\"\n",
    "\n",
    "contents = [\n",
    "    types.Content(role=\"user\", parts=[types.Part(text=query)])\n",
    "]\n",
    "\n",
    "response = gem_client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=contents,\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "print(response.candidates[0].content.parts[0].function_call)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: Execute and respond.** Extract the function call, run it, and send results back for the final response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weather in Sydney is 25.5 degrees Celsius.\n"
     ]
    }
   ],
   "source": [
    "# Check if Gemini called the function\n",
    "if response.candidates[0].content.parts[0].function_call:\n",
    "    tool_call = response.candidates[0].content.parts[0].function_call\n",
    "    \n",
    "    # Execute the actual function with Gemini's provided arguments\n",
    "    result = get_weather(**tool_call.args)\n",
    "    \n",
    "    # Format result for Gemini\n",
    "    function_response_part = types.Part.from_function_response(\n",
    "        name=tool_call.name,\n",
    "        response={\"result\": result}\n",
    "    )\n",
    "    \n",
    "    # Add function call and result to conversation\n",
    "    contents.append(response.candidates[0].content)\n",
    "    contents.append(types.Content(role=\"user\", parts=[function_response_part]))\n",
    "    \n",
    "    # Call model again with function result\n",
    "    final_response = gem_client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        contents=contents,\n",
    "        config=config,\n",
    "    )\n",
    "    \n",
    "    print(final_response.text)\n",
    "else:\n",
    "    # Model responded directly without calling function\n",
    "    print(\"No function call - direct response:\")\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enriching Data with Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for Hugo Bowne-Anderson...\n",
      "Tavily found 10 results:\n",
      "1. https://hugobowne.github.io/hugo-blog/ - Hugo's blog\n",
      "2. https://x.com/hugobowne?lang=en - Hugo Bowne-Anderson (@hugobowne) / Posts / X\n",
      "3. https://au.linkedin.com/in/hugo-bowne-anderson-045939a5 - Hugo Bowne-Anderson - Data and AI scientist, consultant. ...\n",
      "4. https://hugobowne.github.io/ - hugo bowne-anderson - data scientist\n",
      "5. https://www.turnthelenspodcast.com/episode/hugo-bowne-anderson-data-scientists-evangelists-easy-button-turn-the-lens-15 - Hugo Bowne-Anderson: Data Scientists, Evangelists, Easy ...\n",
      "6. https://www.youtube.com/watch?v=eC3RNuI6ow0 - How to Build and Evaluate AI systems in the Age of LLMs ...\n",
      "7. https://vanishinggradients.fireside.fm/hosts/hugobowne - Hugo Bowne-Anderson\n",
      "8. https://learnbayesstats.com/episode/122-learning-and-teaching-in-the-age-of-ai-hugo-bowne-anderson - 122 Learning And Teaching In The Age Of Ai Hugo Bowne ...\n",
      "9. https://hugobowne.substack.com/p/rethinking-data-science-ml-and-ai - Rethinking Data Science, ML, and AI - Vanishing Gradients\n",
      "10. https://outerbounds.com/blog/announcing-full-stack-ml-courses - Free Live Courses: Full-Stack Machine Learning and ...\n",
      "Found: https://hugobowne.github.io/hugo-blog/\n"
     ]
    }
   ],
   "source": [
    "from tavily import TavilyClient\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Initialize clients\n",
    "tavily_client = TavilyClient(api_key=os.getenv(\"TAVILY_API_KEY\"))\n",
    "\n",
    "name = \"Hugo Bowne-Anderson\"\n",
    "print(f\"Searching for {name}...\")\n",
    "\n",
    "try:\n",
    "    # 1. GET DATA (Tavily)\n",
    "    query = f\"{name} AI machine learning data science personal website\" \n",
    "    response = tavily_client.search(query, max_results=10)\n",
    "    \n",
    "    # Format results\n",
    "    search_context = []\n",
    "    print(f\"Tavily found {len(response.get('results', []))} results:\")\n",
    "    for idx, result in enumerate(response.get(\"results\", []), 1):\n",
    "        url = result.get('url', '')\n",
    "        title = result.get('title', '')\n",
    "        print(f\"{idx}. {url} - {title}\")\n",
    "        search_context.append(\n",
    "            f\"URL: {url}\\n\"\n",
    "            f\"Title: {title}\\n\"\n",
    "            f\"Content: {result.get('content', '')[:300]}\\n\"\n",
    "            \"---\"\n",
    "        )\n",
    "    full_context = \"\\n\".join(search_context)\n",
    "\n",
    "    # 2. EVALUATE (OpenAI GPT-4o-mini)\n",
    "    prompt = f\"\"\"\n",
    "    I am looking for the OFFICIAL personal website for: {name}\n",
    "\n",
    "    IMPORTANT CONTEXT: This person works in Data Science, AI, or Machine Learning. \n",
    "\n",
    "    If a website belongs to someone with the same name but a different profession (e.g., a lawyer or doctor), REJECT IT.\n",
    "\n",
    "    Here are the search results:\n",
    "    {full_context}\n",
    "\n",
    "    Task:\n",
    "    1. Analyze the results.\n",
    "    2. Identify the official personal website (Priority: Personal Domain > Academic Profile > LinkedIn/Twitter).\n",
    "    3. Verify they are in the AI/ML/Data field.\n",
    "    4. If no valid website is found, return null.\n",
    "\n",
    "    Output JSON format: {{ \"url\": \"https://...\" or null }}\n",
    "    \"\"\"\n",
    "\n",
    "    llm_response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        response_format={\"type\": \"json_object\"}\n",
    "    )\n",
    "    \n",
    "    # 3. PARSE\n",
    "    result_json = json.loads(llm_response.choices[0].message.content)\n",
    "    found_url = result_json.get(\"url\")\n",
    "\n",
    "    if found_url:\n",
    "        print(f\"Found: {found_url}\")\n",
    "    else:\n",
    "        print(f\"No good match found.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error processing {name}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You've now seen function calling with both OpenAI and Gemini. The core pattern is the same across providers:\n",
    "- Define functions and describe them to the model\n",
    "- Model decides when to call them and with what arguments\n",
    "- You execute the actual function\n",
    "- Results go back to the model for a final response\n",
    "\n",
    "**Key insight:** The model doesn't execute functions: it just requests them. You control what actually runs.\n",
    "\n",
    "**What's next:** Session 6b explores multi-LLM workflows where multiple models coordinate to accomplish complex tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
