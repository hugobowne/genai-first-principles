{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d872d011",
   "metadata": {},
   "source": [
    "## 1. Welcome: Setting the Scene  \n",
    "\n",
    "![LLM SDLC Overview](img/LLM-SDLC_Fig1_edit3-1.png)\n",
    "\n",
    "In this session, we're building the **first iteration** of an app that queries text files. We'll start with **LlamaIndex** to get something working quickly, but then we will **peel back the layers** to see exactly how it works.\n",
    "\n",
    "**The Catch:** While LlamaIndex makes building fast, it hides the complexity. You won't see the prompts or retrieval logic. This is \"Proof-of-Concept Purgatory.\"\n",
    "\n",
    "**What We'll Build:**\n",
    "- **App 1 & 2:** A query app using LlamaIndex abstractions.\n",
    "- **App 3 & 4:** A **Vanilla Python** version where we control every line of code.\n",
    "- **App 5:** Adding **Logging and Observability** with SQLite.\n",
    "- **App 6:** A **Multimodal** app that can read PDFs directly (images and all).\n",
    "\n",
    "**By the end:** You'll have built an AI app three different ways: using a framework, using raw Python, and using multimodal capabilities.\n",
    "\n",
    "ðŸ‘‰ **By the end of this session, you will have:**  \n",
    "- A working LLM-powered app.\n",
    "- A **Gradio frontend** for uploading PDFs.\n",
    "- **Logging** to track every query and response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38229921",
   "metadata": {},
   "source": [
    "## 2. Setup: API Keys  \n",
    "\n",
    "![AI Studio](img/ai-studio.png)\n",
    "\n",
    "### Set Up API Keys  \n",
    "Before running the apps, ensure your **API keys** are configured:\n",
    "\n",
    "1. **Get your API keys:**\n",
    "   - Google Gemini: [AI Studio](https://aistudio.google.com/)\n",
    "   - OpenAI: [Platform](https://platform.openai.com/) (optional)\n",
    "   - Anthropic: [Console](https://console.anthropic.com/) (optional)\n",
    "\n",
    "2. **Configure your environment:**\n",
    "   - Copy `.env.example` to `.env`: `cp .env.example .env`\n",
    "   - Add your API keys to the `.env` file\n",
    "   - Source the file: `source .env`\n",
    "\n",
    "See the main README.md for detailed setup instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c6b051",
   "metadata": {},
   "source": [
    "## 3. Exploring the First App: Querying Text Files  \n",
    "\n",
    "![LlamaIndex](img/LlamaIndex.png)\n",
    "\n",
    "### What You'll Do  \n",
    "- Start with `1-app-query.py` to build the **core query engine** for LinkedIn text profiles.\n",
    "- The app will load `.txt` files from the `/data` directory and allow simple querying.  \n",
    "\n",
    "### How It Works  \n",
    "We're using **LlamaIndex** to handle the complexity for us. This is the \"Magic Box\" approach:\n",
    "1. **Load documents:** `SimpleDirectoryReader` loads text files from the `/data` directory.\n",
    "2. **Process and index:** LlamaIndex creates embeddings and builds a vector index (we don't see how this works).\n",
    "3. **Query:** When you ask a question, LlamaIndex retrieves relevant context and sends it to the LLM.\n",
    "\n",
    "**What We Configure:**\n",
    "- By default, this uses **OpenAI** (`gpt-3.5-turbo` or `gpt-4o` depending on your key/defaults) for both embeddings and generation.\n",
    "\n",
    "**What We Can't See:**\n",
    "- How LlamaIndex constructs the final prompt.\n",
    "- What context gets retrieved from your files.\n",
    "- The exact instructions sent to the model.\n",
    "\n",
    "ðŸ‘‰ **Run the App:**\n",
    "python apps/1-app-query.py**Alternative Models:**\n",
    "If you prefer to use other providers, we have set up alternative scripts:\n",
    "- `1a-app-query-gemini.py` (uses Google Gemini)\n",
    "- `1b-app-query-claude.py` (uses Anthropic Claude)\n",
    "\n",
    "**Reflection:**  \n",
    "You got an answer, but do you know *why*? In the next steps, we will peel back these layers to understand exactly what is happening under the hood."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b681d9",
   "metadata": {},
   "source": [
    "## 4. Adding Interactivity: Gradio Frontend (App 2)  \n",
    "\n",
    "![Gradio App](img/gradio-app.png)\n",
    "\n",
    "Now that the query engine works, let's wrap it in a user interface.  \n",
    "- We'll use `2-app-front-end.py` to create a **Gradio frontend**.  \n",
    "- This allows users to upload PDF files and ask questions via a web browser.  \n",
    "\n",
    "### How It Works  \n",
    "1. **PDF Upload:** Users upload PDF files through the Gradio interface.\n",
    "2. **Text Extraction:** The app uses `PyMuPDF` to extract text from the uploaded PDF.\n",
    "3. **Dynamic Indexing:** A new LlamaIndex vector index is created on the fly for the uploaded file.\n",
    "4. **Query Interface:** Users type questions and get responses.\n",
    "\n",
    "**Key Difference:**\n",
    "Unlike App 1, which pre-loaded files from a folder, this version builds the index *dynamically* when you upload a file.\n",
    "\n",
    "**Run the App:**\n",
    "\n",
    "```bash\n",
    "python apps/2-app-front-end.py\n",
    "```\n",
    "\n",
    "*Click the local URL (e.g., http://127.0.0.1:7860) to open the interface.*\n",
    "\n",
    "**The Problem:**  \n",
    "While this looks like a complete app, we are still in \"Proof of Concept Purgatory.\" We have no logs, we don't know what the model is actually seeing, and debugging is difficult."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6946e04",
   "metadata": {},
   "source": [
    "## 5. Peeling Back the Layers: Vanilla Python + LLM API (App 3)\n",
    "\n",
    "Now we strip away LlamaIndex. We want to see **exactly** what we are sending to the model.\n",
    "\n",
    "In `3-vanilla-python-query.py`, we implement the \"RAG\" pattern manually:\n",
    "1.  **Read the file:** We manually open and read `data/o1.txt`.\n",
    "2.  **Construct the Prompt:** We use a Python f-string to insert the text directly into a prompt template.\n",
    "3.  **Call the API:** We send that specific string to Gemini directly.\n",
    "\n",
    "### Why do this?\n",
    "This gives us **determinism** and **control**. We know exactly what context the model has.\n",
    "\n",
    "**Run the App:**\n",
    "python apps/3-vanilla-python-query.py**Look at the code:** Open `apps/3-vanilla-python-query.py`. Notice how the `prompt` variable is constructed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bf0771",
   "metadata": {},
   "source": [
    "## 6. Rebuilding the Frontend:  Vanilla Python + LLM API + Gradio (App 4)\n",
    "\n",
    "Now let's put our \"Vanilla\" logic back behind a web interface.\n",
    "\n",
    "In `4-vanilla-gradio-query.py`, we combine:\n",
    "- **Gradio** (for the UI)\n",
    "- **PyMuPDF** (for explicit text extraction)\n",
    "- **Gemini API** (direct calls)\n",
    "\n",
    "### How It Works\n",
    "1. **Upload:** User uploads a PDF.\n",
    "2. **Extract:** We explicitly loop through pages and extract text using `page.get_text()`.\n",
    "3. **Prompt:** We stuff that text into our f-string prompt.\n",
    "4. **Generate:** We get the answer.\n",
    "\n",
    "**Run the App:**\n",
    "python `apps/4-vanilla-gradio-query.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c529314",
   "metadata": {},
   "source": [
    "## 7. Logging and Observability (App 5)\n",
    "\n",
    "In `5-vanilla-gradio-sqlite.py`, we extend our vanilla Gradio app by adding SQLite logging to track queries and responses.\n",
    "- This adds observability to the existing web interface.\n",
    "- Allows iterative improvements based on real usage data.\n",
    "\n",
    "### How It Works\n",
    "1. **Database Setup:** SQLite database (`data/interactions.db`) stores each interaction.\n",
    "2. **Log Each Query:** Every PDF upload, query, and response is logged with metadata.\n",
    "3. **Rich Metadata:** Logs include PDF name, timestamp, unique interaction ID, query, and response.\n",
    "\n",
    "**Key Additions:**\n",
    "- **Interaction tracking:** Each query-response pair gets a unique ID and timestamp.\n",
    "- **PDF context:** Logs which PDF was queried, helping identify document-specific patterns.\n",
    "\n",
    "**Run the App:**\n",
    "`python apps/5-vanilla-gradio-sqlite.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31064ee",
   "metadata": {},
   "source": [
    "\n",
    "## 8. Visualizing with Datasette  \n",
    "\n",
    "![Datasette](img/datasette.png)\n",
    "\n",
    "To analyze logs, use **Datasette** to query and filter results. Datasette is a tool for exploring and publishing data that makes SQLite databases easy to browse and query through a web interface.\n",
    "\n",
    "```bash\n",
    "datasette data/interactions.db\n",
    "```  \n",
    "- This launches a web interface where you can view all logged interactions\n",
    "- Filter and search queries by PDF name, timestamp, or content\n",
    "- Export data for further analysis if needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adf67c9",
   "metadata": {},
   "source": [
    "## 9. Going Multimodal (App 6)\n",
    "\n",
    "Finally, in `6-multimodal-pdf-extraction.py`, we ditch the manual text extraction entirely.\n",
    "\n",
    "**The Old Way (Apps 2-5):**\n",
    "`PDF -> Python Library (PyMuPDF) -> Text -> LLM`\n",
    "*Problem: We lose charts, images, and layout information.*\n",
    "\n",
    "**The New Way (App 6):**\n",
    "`PDF -> LLM (Gemini 2.5)`\n",
    "*Gemini is **multimodal**, meaning it can \"see\" the PDF file directly, including images, charts, and layouts, without us needing to turn it into text first.*\n",
    "\n",
    "**Run the App:**\n",
    "```bash\n",
    "python apps/6-multimodal-pdf-extraction.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e51e95c",
   "metadata": {},
   "source": [
    "## 10. Key Takeaways  \n",
    "\n",
    "- **Beyond the Magic Box:** We started with LlamaIndex (\"Proof-of-Concept Purgatory\") but then peeled back the layers to see exactly how prompts are constructed.\n",
    "- **Vanilla Control:** We learned that manual implementation (Vanilla Python) gives us determinism and makes debugging easier compared to black-box abstractions.\n",
    "- **Observability:** We built a logging system with SQLite and Datasette to see exactly what users are asking and what the model is returning.\n",
    "- **Multimodal capabilities:** We saw how modern models (like Gemini 1.5/2.5) can natively read PDFs, preserving context like charts and images that simple text extractors miss."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
