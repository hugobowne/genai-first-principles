{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 6a: Function Calling with LLM APIs\n",
    "\n",
    "## What This Covers\n",
    "\n",
    "Function calling (also called \"tool use\") lets LLMs interact with external systems: APIs, databases, calculators, or any Python function you define. Instead of just generating text, the model can request function calls with specific parameters, you execute them, and return results.\n",
    "\n",
    "This notebook shows:\n",
    "- **OpenAI function calling**: Weather API example with explicit execution flow\n",
    "- **Gemini function calling**: Same pattern using Google's API\n",
    "- **Key pattern**: Model decides → You execute → Model generates final response\n",
    "\n",
    "**The Core Idea:** You describe functions to the model. When appropriate, the model returns structured JSON saying \"call this function with these arguments.\" You execute it and feed results back. The model then uses that information to respond to the user.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- Define function schemas for OpenAI and Gemini\n",
    "- Handle the full execution cycle (request → execute → respond)\n",
    "- Understand when the model calls functions vs. responding directly\n",
    "- Work with real APIs (weather data example)\n",
    "\n",
    "**What's Next:** Session 6b explores LLM workflows where multiple LLMs coordinate using tools.\n",
    "\n",
    "**By the end:** You'll understand how to connect LLMs to external systems and handle the execution flow for both OpenAI and Gemini."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Function Calling\n",
    "\n",
    "We'll start with OpenAI's function calling. The pattern:\n",
    "1. Define a function (e.g., `get_weather`)\n",
    "2. Describe it to the model using a schema\n",
    "3. Call the model with the function available\n",
    "4. Execute the function with the model's provided arguments\n",
    "5. Send results back for the final response\n",
    "\n",
    "Let's see it in action with a weather API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: Define the function.** We'll create `get_weather` that fetches temperature data from a free weather API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from openai import OpenAI\n",
    "import json\n",
    "import os\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = 'XXX'\n",
    "\n",
    "def get_weather(latitude, longitude):\n",
    "    \"\"\"Fetch current temperature from Open-Meteo API.\"\"\"\n",
    "    response = requests.get(\n",
    "        f\"https://api.open-meteo.com/v1/forecast?\"\n",
    "        f\"latitude={latitude}&longitude={longitude}&current=temperature_2m\"\n",
    "    )\n",
    "    data = response.json()\n",
    "    return data['current']['temperature_2m']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Describe the function to the model.** We create a schema that tells OpenAI what the function does and what parameters it expects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "tools = [{\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"get_weather\",\n",
    "        \"description\": \"Get current temperature for provided coordinates in celsius.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"latitude\": {\"type\": \"number\"},\n",
    "                \"longitude\": {\"type\": \"number\"}\n",
    "            },\n",
    "            \"required\": [\"latitude\", \"longitude\"],\n",
    "            \"additionalProperties\": False\n",
    "        },\n",
    "        \"strict\": True\n",
    "    }\n",
    "}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: Call the model.** Send a query to OpenAI with the function schema available. The model \"decides\" whether to call the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatCompletionMessageFunctionToolCall(id='call_bVUfx3rjGgoTKhv6e7hkIlTA', function=Function(arguments='{\"latitude\":-33.8688,\"longitude\":151.2093}', name='get_weather'), type='function')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What's the weather like in Sydney today?\"\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": query }]\n",
    "\n",
    "# Pass tools parameter so model can choose to call get_weather\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-5-mini\",\n",
    "    messages=messages,\n",
    "    tools=tools,\n",
    ")\n",
    "\n",
    "completion.choices[0].message.tool_calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4: Execute the function.** Extract the function call from the model's response and run it with the provided arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the function name and arguments from model's response\n",
    "tool_call = completion.choices[0].message.tool_calls[0]\n",
    "args = json.loads(tool_call.function.arguments)\n",
    "\n",
    "# Execute the actual function\n",
    "result = get_weather(args[\"latitude\"], args[\"longitude\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5: Send results back.** Add the function result to the conversation and call the model again for the final user-facing response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add both the model's tool call and the result to conversation history\n",
    "messages.append(completion.choices[0].message)  \n",
    "messages.append({\n",
    "    \"role\": \"tool\",\n",
    "    \"tool_call_id\": tool_call.id,\n",
    "    \"content\": str(result)\n",
    "})\n",
    "\n",
    "# Call model again with the function result available\n",
    "completion_2 = client.chat.completions.create(\n",
    "    model=\"gpt-5-mini\",\n",
    "    messages=messages,\n",
    "    tools=tools,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Right now in Sydney (CBD) the temperature is 32.2°C — about 90.0°F. \\n\\nI only have the current temperature available. If you’d like I can also get more details (conditions, humidity, wind, hourly or daily forecast) or suggestions for what to wear/bring. Which would you prefer?'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion_2.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gemini Function Calling\n",
    "\n",
    "The same pattern works with Gemini's API, just with different syntax:\n",
    "1. Define and configure the function as a tool\n",
    "2. Call the model with the tool available\n",
    "3. Execute the function and send results back for the final response\n",
    "\n",
    "We'll use the same `get_weather` function and query defined above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import Gemini and create the client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "os.environ['GEMINI_API_KEY'] = 'XXX'\n",
    "\n",
    "client = genai.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: Define and configure the tool.** Create the function schema and wrap it as a Gemini tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the same get_weather function defined above\n",
    "get_weather_tool = {\n",
    "    \"name\": \"get_weather\",\n",
    "    \"description\": \"Get current temperature for provided coordinates in celsius. When the user asks about weather in a city you should determine the latitude and longitude coordinates for that city.\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"latitude\": {\"type\": \"number\"},\n",
    "            \"longitude\": {\"type\": \"number\"}\n",
    "        },\n",
    "        \"required\": [\"latitude\", \"longitude\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "tools = types.Tool(function_declarations=[get_weather_tool])\n",
    "config = types.GenerateContentConfig(tools=[tools])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Call the model.** Send the query with tools available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id=None args={'longitude': 151.2093, 'latitude': -33.8688} name='get_weather'\n"
     ]
    }
   ],
   "source": [
    "query = \"What's the weather like in Sydney today?\"\n",
    "\n",
    "contents = [\n",
    "    types.Content(role=\"user\", parts=[types.Part(text=query)])\n",
    "]\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=contents,\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "print(response.candidates[0].content.parts[0].function_call)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: Execute and respond.** Extract the function call, run it, and send results back for the final response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weather in Sydney is 24 degrees Celsius.\n"
     ]
    }
   ],
   "source": [
    "# Check if Gemini called the function\n",
    "if response.candidates[0].content.parts[0].function_call:\n",
    "    tool_call = response.candidates[0].content.parts[0].function_call\n",
    "    \n",
    "    # Execute the actual function with Gemini's provided arguments\n",
    "    result = get_weather(**tool_call.args)\n",
    "    \n",
    "    # Format result for Gemini\n",
    "    function_response_part = types.Part.from_function_response(\n",
    "        name=tool_call.name,\n",
    "        response={\"result\": result}\n",
    "    )\n",
    "    \n",
    "    # Add function call and result to conversation\n",
    "    contents.append(response.candidates[0].content)\n",
    "    contents.append(types.Content(role=\"user\", parts=[function_response_part]))\n",
    "    \n",
    "    # Call model again with function result\n",
    "    final_response = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        contents=contents,\n",
    "        config=config,\n",
    "    )\n",
    "    \n",
    "    print(final_response.text)\n",
    "else:\n",
    "    # Model responded directly without calling function\n",
    "    print(\"No function call - direct response:\")\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You've now seen function calling with both OpenAI and Gemini. The core pattern is the same across providers:\n",
    "- Define functions and describe them to the model\n",
    "- Model decides when to call them and with what arguments\n",
    "- You execute the actual function\n",
    "- Results go back to the model for a final response\n",
    "\n",
    "**Key insight:** The model doesn't execute functions: it just requests them. You control what actually runs.\n",
    "\n",
    "**What's next:** Session 6b explores multi-LLM workflows where multiple models coordinate to accomplish complex tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
