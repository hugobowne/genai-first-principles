{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d872d011",
   "metadata": {},
   "source": [
    "## 1. Welcome â€“ Setting the Scene  \n",
    "\n",
    "![LLM SDLC Overview](img/LLM-SDLC_Fig1_edit3-1.png)\n",
    "\n",
    "In this session, we're building the **first iteration** of an app that queries LinkedIn profile text files. We'll use **LlamaIndex** to get something working quickly: it handles embeddings, indexing, and retrieval for us, so we can focus on building the app.\n",
    "\n",
    "**The Catch:** While LlamaIndex makes building fast, it hides what's happening behind the scenes. You won't see the prompts it constructs or how it retrieves context: you just get answers. This is what we'll call \"proof-of-concept purgatory\": you have a working app, but you can't see or control the core logic.\n",
    "\n",
    "**What We'll Build:**\n",
    "- A working LLM-powered query app\n",
    "- An interactive **Gradio frontend** for uploading PDFs\n",
    "- **Logging and observability** with SQLite and Datasette\n",
    "- Optional: **Deployment to Modal** â€“ Ship your app so it's available even when your laptop is closed\n",
    "\n",
    "\n",
    "**By the end:** You'll have all the moving parts of an AI app (frontend, backend, logging, visualization), but you'll also experience the frustration of working with abstractions you can't inspect.\n",
    "\n",
    "**What's Next:** The optional homework will have you rebuild without LlamaIndex to see the actual prompts. In the next session, we'll dive into prompt engineering so you can control what the model sees.\n",
    "\n",
    "ðŸ‘‰ **By the end of this session, you will have:**  \n",
    "- A working LLM-powered app that queries text files.  \n",
    "- A simple **Gradio frontend** for uploading PDFs.  \n",
    "- Basic **logging** and **observability** using SQLite and Datasette."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38229921",
   "metadata": {},
   "source": [
    "## 2. Setup â€“ API Keys  \n",
    "\n",
    "![AI Studio](img/ai-studio.png)\n",
    "\n",
    "### Set Up API Keys  \n",
    "Before running the apps, ensure your **API keys** are configured:\n",
    "\n",
    "1. **Get your API keys:**\n",
    "   - Google Gemini: [AI Studio](https://aistudio.google.com/)\n",
    "   - OpenAI: [Platform](https://platform.openai.com/) (optional)\n",
    "   - Anthropic: [Console](https://console.anthropic.com/) (optional)\n",
    "\n",
    "2. **Configure your environment:**\n",
    "   - Copy `.env.example` to `.env`: `cp .env.example .env`\n",
    "   - Add your API keys to the `.env` file\n",
    "   - Source the file: `source .env`\n",
    "\n",
    "See the main README.md for detailed setup instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c6b051",
   "metadata": {},
   "source": [
    "## 3. Exploring the First App â€“ Querying Text Files  \n",
    "\n",
    "![LlamaIndex](img/LlamaIndex.png)\n",
    "\n",
    "### What You'll Do  \n",
    "- Start with `1a-app-query-gemini.py` to build the **core query engine** for LinkedIn text profiles using Google Gemini.  \n",
    "- The app will load `.txt` files from the `/data` directory and allow simple querying.  \n",
    "\n",
    "### How It Works  \n",
    "LlamaIndex handles the complexity for us:\n",
    "1. **Load documents:** `SimpleDirectoryReader` loads text files from the `/data` directory\n",
    "2. **Process and index:** LlamaIndex creates embeddings and builds a vector index (we don't see how this works)\n",
    "3. **Query:** When you ask a question, LlamaIndex retrieves relevant context and sends it to the LLM (we don't see the prompt it constructs)\n",
    "\n",
    "**What We Configure:**\n",
    "- **Embedding model:** `text-embedding-004` (Gemini) for converting text to vectors\n",
    "- **LLM:** `gemini-2.5-flash` for generating answers\n",
    "\n",
    "**What We Can't See:**\n",
    "- How LlamaIndex constructs the final prompt from your query and retrieved context\n",
    "- What context gets retrieved and how\n",
    "- The exact instructions sent to the model\n",
    "\n",
    "ðŸ‘‰ See `1a-app-query-gemini.py` for the full implementation.\n",
    "\n",
    "**Alternative Models:**\n",
    "- Use `1-app-query.py` for OpenAI (default LlamaIndex settings)\n",
    "- Use `1b-app-query-claude.py` for Anthropic Claude\n",
    "- **Note:** To use Anthropic, you'll also need an OpenAI API key because Anthropic doesn't provide embeddings. LlamaIndex will use OpenAI embeddings with Claude as the LLM.\n",
    "\n",
    "**Reflection Question:** What prompt was actually sent to the LLM? Check the code and think about how LlamaIndex constructs the final prompt from your query and the retrieved context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b681d9",
   "metadata": {},
   "source": [
    "## 4. Adding Interactivity â€“ Gradio Frontend (App 2)  \n",
    "\n",
    "![Gradio App](img/gradio-app.png)\n",
    "\n",
    "Once the query engine works, you'll expand by adding a **Gradio frontend** in `2-app-front-end.py`.  \n",
    "- Gradio provides a web interface where users can upload PDFs and type queries directly.  \n",
    "- This makes the app accessible without requiring command-line interaction.  \n",
    "\n",
    "### How It Works  \n",
    "1. **PDF Upload:** Users upload PDF files through the Gradio interface\n",
    "2. **Text Extraction:** PyMuPDF extracts text from the uploaded PDF\n",
    "3. **Dynamic Indexing:** A new vector index is created for each uploaded PDF (unlike App 1 which pre-loads files)\n",
    "4. **Query Interface:** Users type questions and get responses through the web UI\n",
    "\n",
    "**Key Differences from App 1:**\n",
    "- **File upload handling:** Accepts PDFs directly via web interface\n",
    "- **Per-document indexing** Creates a fresh index for each upload rather than loading pre-indexed files\n",
    "- **Web-based UI:** Uses `gr.Blocks()` for a more structured interface with file upload, query input, and response display\n",
    "\n",
    "ðŸ‘‰ See `2-app-front-end.py` for the full implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c529314",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Logging and Observability â€“ App 3  \n",
    "\n",
    "![SQLite](img/SQLITE.jpeg)\n",
    "\n",
    "In `3-app-log.py`, you'll extend the Gradio app from App 2 by adding SQLite logging to track queries and responses.  \n",
    "- This adds observability to the existing web interface.  \n",
    "- Allows iterative improvements based on real usage data.  \n",
    "\n",
    "### How It Works  \n",
    "1. **Database Setup:** SQLite database (`pdf_qa_logs.db`) stores each interaction\n",
    "2. **Log Each Query:** Every PDF upload, query, and response is logged with metadata\n",
    "3. **Rich Metadata:** Logs include PDF name, timestamp, unique interaction ID, query, and response\n",
    "4. **Analysis:** Use Datasette to visualize and analyze patterns in the logged data\n",
    "\n",
    "**Key Additions:**\n",
    "- **Interaction tracking:** Each query-response pair gets a unique ID and timestamp\n",
    "- **PDF context:** Logs which PDF was queried, helping identify document-specific patterns\n",
    "- **Query logging:** Stores both the query and the LLM's response for analysis\n",
    "\n",
    "ðŸ‘‰ See `3-app-log.py` for the full implementation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31064ee",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Visualizing with Datasette  \n",
    "\n",
    "![Datasette](img/datasette.png)\n",
    "\n",
    "To analyze logs, use **Datasette** to query and filter results. Datasette is a tool for exploring and publishing data that makes SQLite databases easy to browse and query through a web interface.\n",
    "\n",
    "```bash\n",
    "datasette pdf_qa_logs.db\n",
    "```  \n",
    "- This launches a web interface where you can view all logged interactions\n",
    "- Filter and search queries by PDF name, timestamp, or content\n",
    "- Export data for further analysis if needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e51e95c",
   "metadata": {},
   "source": [
    "## 7. Key Takeaways  \n",
    "\n",
    "- **Built the MVP:** A simple LLM-powered app that queries LinkedIn text profiles.  \n",
    "- **Added Interactivity:** Introduced Gradio to create a user-friendly interface for querying PDFs.  \n",
    "- **Started Logging:** Integrated SQLite to track queries and responses, preparing for observability and testing.  \n",
    "- **Visualized Logs:** Used Datasette to explore logged data, reinforcing observability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e1d0e75d011767",
   "metadata": {},
   "source": [
    "## Optional: Deploying the Gradio App on Modal  \n",
    "\n",
    "![Modal](img/modal.jpg)\n",
    "\n",
    "Now that we've built the Gradio app for querying PDFs, let's deploy it to **Modal**. This allows you to run the app from anywhere without managing local infrastructure.  \n",
    "\n",
    "### Why Deploy to Modal?  \n",
    "- **Fast Deployment:** Modal simplifies deploying Python apps with minimal configuration.  \n",
    "- **Scalable:** Deploy ephemeral development servers or durable production apps.  \n",
    "- **$500 in Modal Credits:** As part of this course, you'll receive **$500 in Modal credits**.  \n",
    "\n",
    "### Prerequisites:  \n",
    "- **Modal Account:** Sign up at [modal.com](https://modal.com) and redeem your credits using the provided form.  \n",
    "- **Modal SDK:** Pre-installed in the Codespace.  \n",
    "\n",
    "### Deployment Steps:  \n",
    "\n",
    "0. **Authenticate with Modal (first time only):**  \n",
    "   ```bash\n",
    "   modal token new\n",
    "   ```  \n",
    "   This will open a browser for authentication.\n",
    "\n",
    "1. **Set Up Gemini API Key:**  \n",
    "   Make sure you're logged in to Modal, then go to [https://modal.com/secrets/](https://modal.com/secrets/) and create a new secret:\n",
    "   - Secret name: `gemini`\n",
    "   - Add environment variable: `GOOGLE_API_KEY` = your Gemini API key\n",
    "\n",
    "2. **Navigate to the Deployment Directory:**  \n",
    "   Go to the `workshops/workshop-1/apps/deploy/` directory where the deployment script is located.  \n",
    "\n",
    "3. **Run the Gradio App:**  \n",
    "   For an ephemeral development server:  \n",
    "   ```bash\n",
    "   modal serve modal_wrapper\n",
    "   ```  \n",
    "   \n",
    "   For durable deployment:  \n",
    "   ```bash\n",
    "   modal deploy modal_wrapper\n",
    "   ```  \n",
    "\n",
    "### Testing the Deployment:  \n",
    "- Open the URL provided by Modal and test the app by uploading PDFs and running queries.\n",
    "- You can also view your running app and logs at [https://modal.com/apps](https://modal.com/apps) (make sure you're logged in).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826bcb32",
   "metadata": {},
   "source": [
    "### Homework: Direct API Calls Without Frameworks\n",
    "\n",
    "As we noted above, frameworks like LlamaIndex obscure the actual prompt sent to the LLM. This exercise will help you see what's really happening and give you direct control over the prompt.\n",
    "\n",
    "**The Goal:** Build a version of your Gradio app that sends prompts directly to the LLM API without using frameworks. You'll manually construct the prompt using the PDF text and user query, send it to the model, and log the full prompt to your database.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. **Set up your API key:** Configure your environment with the API key for your chosen LLM provider (Gemini, OpenAI, or Claude).\n",
    "\n",
    "2. **Test the Direct API Call:** Before building the full app, verify you can call the LLM API directly. Send a simple query to make sure your API key works and you get a response.\n",
    "   - For Gemini: [Google AI Python SDK docs](https://ai.google.dev/gemini-api/docs/quickstart?lang=python)\n",
    "   - For OpenAI: [OpenAI Python SDK docs](https://platform.openai.com/docs/quickstart)\n",
    "   - For Anthropic Claude: [Anthropic Python SDK docs](https://docs.anthropic.com/en/api/getting-started)\n",
    "\n",
    "3. **Read in local text data:** Load a text file (like `apps/data/hbaLI.txt` from the workshop) to use as test document content. Print it to see what you're working with.\n",
    "\n",
    "4. **Combine the document and query in a prompt:** Construct a prompt that includes both your test document text and a user query. Send this to the LLM and see how it responds with context.\n",
    "\n",
    "5. **Build the Gradio interface:** Create a simple web UI that accepts PDF uploads and user queries. Extract text from the uploaded PDF using PyMuPDF.\n",
    "\n",
    "6. **Add full prompt logging:** Extend your Gradio app to log every interaction to SQLite, storing the **complete prompt** (not just the user query), the response, and metadata like PDF name and timestamp.\n",
    "\n",
    "7. **Use Datasette to explore the logs:** View your logged interactions and compare the `query` column (what the user typed) with the `full_prompt` column (what the model actually received). This is the key insight.\n",
    "\n",
    "This is the moment you stop relying on hidden behavior and start making your own decisions about what the model sees.\n",
    "\n",
    "**Query vs. Prompt:**\n",
    "- **Query:** What the user types into your app (e.g., \"What is this document about?\")\n",
    "- **Prompt:** What you send to the model (instructions + document + query)\n",
    "\n",
    "The query is user input. The prompt is model input. When building with LLMs, **the prompt is the logic**. You need to see it, own it, and iterate on it.\n",
    "\n",
    "**For Reference:**\n",
    "[\"F**k you, show me the prompt\" by Hamel Husain (Parlance Labs)](https://hamel.dev/blog/posts/prompt/): This blog post captures the mindset shift this homework is about."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43c6607",
   "metadata": {},
   "source": [
    "\n",
    "Please spend a few minutes providing feedback on this session in the survey shared in Discord."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
