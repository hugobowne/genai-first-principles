{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Session 1: Building Gen-AI Applications from First Principles\n",
    "\n",
    "Welcome to Session 1 of our workshop on building generative AI applications! Today, we‚Äôll take a comprehensive dive into constructing AI-powered systems using a first-principles approach to break down complex ideas into practical, understandable components. By the end of this session, you'll have hands-on experience developing a generative AI app and a solid understanding of foundational AI concepts.\n",
    "\n",
    "![Alt text](img/3-llm-virtuous-cycle.png)\n",
    "\n",
    "## Here‚Äôs what we‚Äôll cover today:\n",
    "\n",
    "- ‚öôÔ∏è **Setup and Initial Exploration**:  \n",
    "  - Ensure everyone‚Äôs environment is ready for development.\n",
    "  - Run and interact with a foundational generative AI app that queries PDFs and documents to generate responses.\n",
    "\n",
    "- üîÑ **Understanding the Software Development Lifecycle (SDLC) for AI Applications**:  \n",
    "  - Explore the SDLC as an iterative, non-deterministic process, emphasizing the importance of continuous evaluation, logging, and iteration.\n",
    "\n",
    "- üõ†Ô∏è **Core Concepts and Deconstruction**:  \n",
    "  - **Retrieval-Augmented Generation (RAG) Systems**: Discuss how these systems work and why they‚Äôre useful.\n",
    "  - **Breaking Down the MVP**: Walk through each component of the app, explaining how it fits into the broader AI workflow.\n",
    "\n",
    "- üí° **Vendor APIs and Prompt Engineering**:  \n",
    "  - Explore major vendor APIs (e.g., OpenAI‚Äôs APIs) and the differences between endpoints like Completion and Chat.\n",
    "  - Experiment with prompt engineering: adjusting prompts to optimize output and understanding the nuances of input-output relationships.\n",
    "\n",
    "- üìÑ **Structured Outputs**:  \n",
    "  - Discuss the importance of extracting structured outputs from unstructured data.\n",
    "  - Use the example of querying LinkedIn PDFs to extract structured information, such as professional details from a LinkedIn profile.\n",
    "\n",
    "- üîç **Deep Dive into Embeddings and Vector Stores**:  \n",
    "  - **What is an Embedding?**: Learn how embeddings represent data and why they‚Äôre foundational to generative AI.\n",
    "  - **Vector Stores**: Briefly explain how vector stores work, focusing on their role in providing context and structuring data.\n",
    "  - Discuss the impact of embeddings on output quality and how they reflect different ‚Äúworld models.‚Äù\n",
    "\n",
    "- üìù **Logging and Evaluation**:  \n",
    "  - **Logging Interactions**: Set up a simple logging system using SQLite to track queries and responses for future analysis.\n",
    "  - **Evaluating Output**: Introduce basic evaluation strategies, like thumbs-up/thumbs-down feedback, and explain why evaluation is crucial.\n",
    "  - Provide guidance on where to learn more, such as blogs or tutorials from well-known experts in the field.\n",
    "\n",
    "## Interactive Exercises:\n",
    "\n",
    "- üèóÔ∏è **Hands-on Practice**:  \n",
    "  - Experiment with the app: querying documents, tuning prompts, and observing output changes.\n",
    "  - Work on extracting structured information from LinkedIn PDFs.\n",
    "  - Implement logging and evaluation mechanisms.\n",
    "  - Explore how different embeddings affect responses and think critically about these representations.\n",
    "\n",
    "By the end of today‚Äôs session, you‚Äôll have a comprehensive understanding of the first principles behind generative AI applications, practical experience building and iterating on an AI app, and the foundation needed to dive deeper in Session 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the First App: Building an Index and Querying Documents\n",
    "\n",
    "In this section, we‚Äôll run our first simple generative AI app. \n",
    "\n",
    "### Setting Up Your Environment\n",
    "\n",
    "Before we run the app, you'll need to set up your OpenAI API key. Follow these steps to ensure everything works smoothly:\n",
    "\n",
    "1. **Obtain Your OpenAI API Key**:\n",
    "   - Go to the [OpenAI website](https://platform.openai.com/signup) and sign up or log in to your account.\n",
    "   - Once logged in, generate an API key from the API settings page.\n",
    "\n",
    "2. **Export the API Key**:\n",
    "   - You‚Äôll need to export your API key as an environment variable. Use the following command in your terminal:\n",
    "     ```bash\n",
    "     export OPENAI_API_KEY=\"your_api_key_here\"\n",
    "     ```\n",
    "   - Make sure to replace `\"your_api_key_here\"` with your actual OpenAI API key.\n",
    "\n",
    "3. **Verify Your Setup**:\n",
    "   - Check that your API key is correctly set by running:\n",
    "     ```bash\n",
    "     echo $OPENAI_API_KEY\n",
    "     ```\n",
    "   - This should print your API key if everything is set up correctly.\n",
    "\n",
    "\n",
    "\n",
    "Once you have your API key configured, you‚Äôre ready to dive into running and interacting with the generative AI app!\n",
    "\n",
    "\n",
    "We‚Äôll use the `llama_index` library to index a set of documents and then query them for information. Here‚Äôs a breakdown of what each line of the code does:\n",
    "\n",
    "### Step-by-Step Explanation\n",
    "\n",
    "**Load and Prepare the Documents**:\n",
    "\n",
    "**Explanation**: We import the necessary classes from `llama_index`. The `SimpleDirectoryReader` is used to load all documents from the `data` directory.\n",
    "\n",
    "**Goal**: Load the data from your documents so that we can build an index for querying.\n",
    "\n",
    "```\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "```\n",
    "\n",
    "**Create the Index**:\n",
    "\n",
    "**Explanation**: We create a `VectorStoreIndex` from the loaded documents. This index allows us to efficiently search through the documents using embeddings.\n",
    "\n",
    "**Goal**: Build an index that can handle search queries.\n",
    "\n",
    "```\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "```\n",
    "\n",
    "**Set Up the Query Engine**:\n",
    "\n",
    "**Explanation**: We convert the index into a query engine. This engine will help us run natural language queries on our indexed documents.\n",
    "\n",
    "**Goal**: Prepare a query engine that can interpret and respond to user questions.\n",
    "\n",
    "```\n",
    "query_engine = index.as_query_engine()\n",
    "```\n",
    "\n",
    "**Query the Documents**:\n",
    "\n",
    "**Explanation**: We use the query engine to ask, ‚Äúwhat is o1,‚Äù and then print the response.\n",
    "\n",
    "**Goal**: See how the generative AI app responds to our question using the indexed data.\n",
    "\n",
    "```\n",
    "response = query_engine.query(\"what is o1\")\n",
    "print(response)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Personalize Your Queries\n",
    "\n",
    "We‚Äôll take this app a step further by making it more personal! Here‚Äôs what you‚Äôll do:\n",
    "\n",
    "1. **Create a Text File**: \n",
    "   - Copy and paste the content from your LinkedIn profile into a text file.\n",
    "   - Name the file something like `my_linkedin_profile.txt` and save it in the `data` directory.\n",
    "   - Make sure to include all the important details, such as your work experience, skills, and education.\n",
    "\n",
    "2. **Ask Questions About Yourself**: \n",
    "   - Once you‚Äôve added your LinkedIn content to the `data` directory, try running the app and ask questions about your own profile.\n",
    "   - For example, you could ask:\n",
    "     - \"What is my most recent job title?\"\n",
    "     - \"What skills do I have listed?\"\n",
    "     - \"Where did I go to university?\"\n",
    "\n",
    "This exercise will help you see how generative AI can interpret and respond to your personal data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Simple RAG System\n",
    "\n",
    "![Simple RAG](img/0-simple-RAG.png)\n",
    "\n",
    "We‚Äôll dive deeper into how this system works later on, but here's a quick overview of what you're looking at:\n",
    "\n",
    "- **Structured and Unstructured Data**: We start with various types of data, like text documents or structured information.\n",
    "- **Chunking**: Data is broken down into manageable pieces, or \"chunks,\" to be processed.\n",
    "- **Vector DB (Embeddings)**: These chunks are then embedded using a text embedding model and stored in a vector database for efficient retrieval.\n",
    "- **Response Generation**: The system retrieves relevant chunks based on your query and uses a large language model to generate a response.\n",
    "\n",
    "The cool thing about this setup is that it abstracts away a lot of the complexity, such as embeddings, vector databases, and chunking. However, as we build and experiment, we need to be mindful of not getting stuck in **POC purgatory**‚Äîwhere we have a great proof-of-concept but never progress to a fully integrated, scalable solution.\n",
    "\n",
    "Stay tuned, as we‚Äôll explore these concepts in more detail!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Iteration: Adding a Front-End with Gradio\n",
    "\n",
    "Now that we‚Äôve built the first version of our app, let‚Äôs make it more user-friendly by adding a front-end with **Gradio**. We‚Äôre also using **PyMuPDF** to handle PDF text extraction.\n",
    "\n",
    "![Alt Text](img/1-gradio-fe.png)\n",
    "\n",
    "**What‚Äôs New in This Iteration**:\n",
    "- **Front-End with Gradio**: We‚Äôve added a simple Gradio interface that allows you to upload a PDF and ask questions about its contents.\n",
    "- **PDF Handling with PyMuPDF**: Instead of manually working with text, we can now extract text directly from PDFs, making the app more versatile.\n",
    "\n",
    "\n",
    "\n",
    "### Key Highlights:\n",
    "\n",
    "1. **Gradio Interface**: With just a few lines of code, we‚Äôve created a user-friendly interface:\n",
    "   - A **file upload** component to drag and drop your LinkedIn PDF.\n",
    "   - A **textbox** to type your questions.\n",
    "   - A **button** to submit your queries and get instant responses.\n",
    "  \n",
    "2. **Minimal Code, Maximum Impact**: \n",
    "   - The Gradio setup is simple yet powerful. It only takes a few lines of code to add an intuitive front-end to our app.\n",
    "   - This demonstrates how easily you can transform a backend script into an interactive AI-powered tool.\n",
    "\n",
    "\n",
    "\n",
    "### Your Task:\n",
    "\n",
    "1. **Create a PDF of Your LinkedIn Profile**:\n",
    "   - Open your LinkedIn profile.\n",
    "   - Click **Print** and then **Save as PDF** to generate a PDF.\n",
    "  \n",
    "2. **Upload and Query**:\n",
    "   - Use the Gradio interface to upload your LinkedIn PDF.\n",
    "   - Ask questions about your profile, like:\n",
    "     - \"What is my job title?\"\n",
    "     - \"What skills do I have listed?\"\n",
    "     - \"Where did I go to university?\"\n",
    "\n",
    "This exercise will help you understand how adding a simple front-end can make your AI app more engaging and accessible. Plus, you‚Äôll get to see the magic of querying your own profile in real time!\n",
    "\n",
    "**Note**: If you‚Äôre curious about the full code, refer to the `2-app-front-end.py` file. Feel free to explore and modify it as you like!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third Iteration: Enhancing with Local Embeddings and Advanced LLM Integration\n",
    "\n",
    "In this version of our app, we‚Äôre taking things up a notch by:\n",
    "- Using **Hugging Face embeddings** for efficient, local text representation.\n",
    "- Incorporating the **Ollama API** to query the PDF with an advanced large language model like **LLaMA2**.\n",
    "\n",
    "These enhancements allow for more flexible and efficient querying, while still keeping the code approachable.\n",
    "\n",
    "\n",
    "\n",
    "### What's New in This Iteration?\n",
    "\n",
    "1. **Local Embeddings with Hugging Face**:\n",
    "   - We‚Äôre now specifying a local embedding model using `HuggingFaceEmbedding`. This gives us more control and flexibility while ensuring data privacy.\n",
    "   - The model we‚Äôre using, `\"sentence-transformers/all-MiniLM-L6-v2\"`, is lightweight and suitable for many use cases, though it might not perform as well as proprietary models like OpenAI‚Äôs embeddings in terms of accuracy and nuance.\n",
    "\n",
    "2. **Ollama API for LLaMA2**:\n",
    "   - We‚Äôve integrated the Ollama API to use **LLaMA2** as our language model, giving us more flexibility in handling complex queries.\n",
    "   - The Ollama integration makes it easy to configure and query the LLM with a simple interface.\n",
    "\n",
    "3. **Considerations for Self-Hosting**:\n",
    "   - You might think that you can just self-host the LLM and call it a day. However, there‚Äôs an important nuance: if you want to self-host the entire pipeline, you also need to set up your own **embedding model**.\n",
    "   - The OpenAI API does something clever: it seamlessly handles both the embedding and the generative task for you. When you switch to self-hosted models, you‚Äôre responsible for managing the embedding step and ensuring it integrates well with your LLM.\n",
    "\n",
    "4. **Improved User Experience**:\n",
    "   - The app now handles edge cases to make sure it runs smoothly:\n",
    "     - **Ensuring a PDF is Uploaded**: This check is done in the `query_pdf` function:\n",
    "       ```python\n",
    "       if pdf is None:\n",
    "           return \"Please upload a PDF.\"\n",
    "       ```\n",
    "       This ensures that the user doesn‚Äôt proceed without uploading a file.\n",
    "     - **Validating the Query**: We also check that the query isn‚Äôt empty:\n",
    "       ```python\n",
    "       if not query.strip():\n",
    "           return \"Please enter a valid query.\"\n",
    "       ```\n",
    "       This makes sure the user provides a meaningful question before processing.\n",
    "\n",
    "\n",
    "\n",
    "### Key Highlights:\n",
    "\n",
    "- **Gradio Setup**: We continue to use Gradio to provide a clean and simple front-end.\n",
    "- **Embeddings and LLM**:\n",
    "  - We‚Äôre using **Hugging Face** for local embeddings, which brings benefits like:\n",
    "    - **Privacy**: Your data stays on your local machine.\n",
    "    - **Self-Hosting**: You‚Äôre not reliant on external APIs.\n",
    "    - **Cost Efficiency**: No need to pay for API calls to third-party services.\n",
    "  - **Trade-Off**: Performance may not be as strong as OpenAI‚Äôs models, and you need to manage more of the setup, like embedding models.\n",
    "\n",
    "\n",
    "\n",
    "### Your Task:\n",
    "\n",
    "1. **Run the Code**: Execute the provided Python code to launch the enhanced version of the app.\n",
    "2. **Upload Your LinkedIn PDF**:\n",
    "   - Use the PDF you created earlier or generate a new one from your LinkedIn profile.\n",
    "3. **Ask More Complex Questions**:\n",
    "   - With these improvements, try asking more detailed or nuanced questions, such as:\n",
    "     - \"Summarize my professional background.\"\n",
    "     - \"List the programming languages I have experience with.\"\n",
    "     - \"What are the key highlights of my career?\"\n",
    "\n",
    "### Experiment and Reflect:\n",
    "\n",
    "- **Play Around with Queries**: Observe how the app handles your queries with the new embedding model and LLM integration. You may notice differences in performance compared to using proprietary APIs like OpenAI.\n",
    "- **Consider the Trade-offs**: Remember, self-hosting comes with more responsibilities. Setting up an LLM isn‚Äôt just about running the model‚Äîyou also need to handle embeddings, which OpenAI did seamlessly for you.\n",
    "\n",
    "**Note**: If you‚Äôre curious about the full code, refer to the `3-app-local.py` file. Feel free to explore and modify it as you like!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fourth Iteration: Introducing Conversational Interaction and Model Choice\n",
    "\n",
    "In this version, we‚Äôre adding a key feature: **conversational memory**. Now, you can have a back-and-forth conversation with the AI to refine your questions and get more precise answers. Additionally, you can choose between a local LLM (Ollama) or OpenAI for generating responses.\n",
    "\n",
    "\n",
    "\n",
    "### What's New in This Iteration?\n",
    "\n",
    "1. **Conversational Memory**:\n",
    "   - The app now maintains a **conversation history**, so it can use context from previous questions to give more relevant answers.\n",
    "   - This is particularly useful when you need to ask follow-up questions or clarify information from the PDF.\n",
    "\n",
    "2. **Model Choice**:\n",
    "   - You can choose between two different models:\n",
    "     - **Local (Ollama)**: Uses a locally hosted model (like LLaMA2) for privacy and self-hosting.\n",
    "     - **OpenAI**: Uses OpenAI‚Äôs GPT-3.5-turbo, which might provide better performance for nuanced questions but requires an API key.\n",
    "   - The model selection is done through a simple **Gradio radio button** interface.\n",
    "\n",
    "\n",
    "\n",
    "### Key Highlights in the Code:\n",
    "\n",
    "1. **Conversation Handling**:\n",
    "   - The `query_pdf` function now takes a `history` parameter to keep track of the conversation.\n",
    "   - Previous interactions are included in the new query to provide context, making the conversation more coherent:\n",
    "     ```python\n",
    "     conversation = \"\\n\".join([f\"User: {h[0]}\\nAssistant: {h[1]}\" for h in history])\n",
    "     conversation += f\"\\nUser: {query}\\n\"\n",
    "     ```\n",
    "\n",
    "2. **Model Selection**:\n",
    "   - You can choose between **Ollama** for a local model or **OpenAI** for a cloud-based option:\n",
    "     ```python\n",
    "     if model_choice == \"Local (Ollama)\":\n",
    "         llm = Ollama(model=\"llama2\", request_timeout=60.0)\n",
    "     elif model_choice == \"OpenAI\":\n",
    "         openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "         llm = OpenAI(api_key=openai_api_key, model=\"gpt-3.5-turbo\")\n",
    "     ```\n",
    "\n",
    "3. **Error Handling**:\n",
    "   - The app now catches exceptions and provides a friendly error message, helping you debug any issues:\n",
    "     ```python\n",
    "     except Exception as e:\n",
    "         return [(\"An error occurred\", str(e))], history\n",
    "     ```\n",
    "\n",
    "\n",
    "\n",
    "### Your Task:\n",
    "\n",
    "1. **Run the Code**: Launch the app and experiment with having a conversation about your LinkedIn PDF.\n",
    "2. **Upload Your PDF**:\n",
    "   - Use the PDF you created earlier or generate a new one from your LinkedIn profile.\n",
    "3. **Ask Follow-Up Questions**:\n",
    "   - Start with a broad question and then ask more specific or clarifying follow-ups. For example:\n",
    "     - \"What are the main highlights of my professional experience?\"\n",
    "     - \"Can you elaborate on my skills related to data science?\"\n",
    "     - \"What is my educational background?\"\n",
    "4. **Switch Models**:\n",
    "   - Experiment with both the **Local (Ollama)** and **OpenAI** models to see how they handle your queries differently.\n",
    "\n",
    "### Considerations:\n",
    "\n",
    "- **Performance Differences**: OpenAI‚Äôs model might perform better with complex, nuanced questions, but using it requires an API key and depends on an external service.\n",
    "- **Self-Hosting Trade-Offs**: The local Ollama model provides more privacy and control but might not be as robust or accurate as OpenAI‚Äôs offerings.\n",
    "\n",
    "\n",
    "\n",
    "This iteration brings your app closer to a conversational AI assistant, capable of refining its responses over multiple interactions. Feel free to test and see how well it handles different queries and models!\n",
    "\n",
    "**Note**: If you‚Äôre curious about the full code, refer to the `4-app-convo-log.py` file. Feel free to explore and modify it as you like!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fifth Iteration: Adding Logging and Observability\n",
    "\n",
    "In this version, we're focusing on **logging** every interaction to a local SQLite database to improve **observability** and **monitoring**. This feature helps us keep track of conversations, queries, and responses, which is essential for debugging, auditing, and improving the app.\n",
    "\n",
    "\n",
    "\n",
    "### What's New in This Iteration?\n",
    "\n",
    "1. **Logging Conversations**:\n",
    "   - We‚Äôre now storing each conversation and message in a local SQLite database (`qa_traces.db`).\n",
    "   - Every interaction is logged, including the user's queries, the assistant's responses, and any system errors. This gives us a complete trace of each session.\n",
    "\n",
    "2. **Database Setup**:\n",
    "   - We use **SQLite** for simplicity and set up the database schema to store:\n",
    "     - **Conversations**: A record for each new conversation, with a unique `conversation_id` and a timestamp.\n",
    "     - **Messages**: Each message in the conversation, including its role (`user` or `assistant`), content, and timestamp.\n",
    "\n",
    "3. **Observability with Datasette**:\n",
    "   - We‚Äôll use **Simon Willison‚Äôs Datasette** to explore the conversation logs.\n",
    "   - Datasette is a powerful tool for inspecting and querying SQLite databases, making it easy to analyze and understand the data.\n",
    "\n",
    "![Alt Text](img/datasette_traces.png)\n",
    "\n",
    "\n",
    "\n",
    "### Key Highlights in the Code:\n",
    "\n",
    "1. **Thread-Local Database Connection**:\n",
    "   - We use `threading.local()` to ensure each thread has its own connection to the SQLite database:\n",
    "     ```python\n",
    "     local = threading.local()\n",
    "     def get_db_connection():\n",
    "         if not hasattr(local, \"db_conn\"):\n",
    "             local.db_conn = sqlite3.connect('qa_traces.db', check_same_thread=False)\n",
    "             # Create tables if they don't exist\n",
    "             local.db_conn.execute('''CREATE TABLE IF NOT EXISTS conversations\n",
    "                                      (id TEXT PRIMARY KEY, timestamp TEXT)''')\n",
    "             local.db_conn.execute('''CREATE TABLE IF NOT EXISTS messages\n",
    "                                      (id TEXT PRIMARY KEY, conversation_id TEXT, \n",
    "                                       timestamp TEXT, role TEXT, content TEXT,\n",
    "                                       FOREIGN KEY(conversation_id) REFERENCES conversations(id))''')\n",
    "             local.db_conn.commit()\n",
    "         return local.db_conn\n",
    "     ```\n",
    "\n",
    "2. **Starting a New Conversation**:\n",
    "   - We create a new conversation ID and log it in the database:\n",
    "     ```python\n",
    "     def start_conversation():\n",
    "         conn = get_db_connection()\n",
    "         c = conn.cursor()\n",
    "         conversation_id = str(uuid.uuid4())\n",
    "         timestamp = datetime.now().isoformat()\n",
    "         c.execute(\"INSERT INTO conversations VALUES (?, ?)\", (conversation_id, timestamp))\n",
    "         conn.commit()\n",
    "         return conversation_id\n",
    "     ```\n",
    "\n",
    "3. **Logging Messages**:\n",
    "   - Each user query and assistant response is logged:\n",
    "     ```python\n",
    "     def log_message(conversation_id, role, content):\n",
    "         conn = get_db_connection()\n",
    "         c = conn.cursor()\n",
    "         message_id = str(uuid.uuid4())\n",
    "         timestamp = datetime.now().isoformat()\n",
    "         c.execute(\"INSERT INTO messages VALUES (?, ?, ?, ?, ?)\", \n",
    "                   (message_id, conversation_id, timestamp, role, content))\n",
    "         conn.commit()\n",
    "     ```\n",
    "\n",
    "4. **Using the Logs for Debugging and Monitoring**:\n",
    "   - If an error occurs during the conversation, we log it as a `system` message:\n",
    "     ```python\n",
    "     log_message(conversation_id, \"system\", f\"Error: {error_message}\")\n",
    "     ```\n",
    "\n",
    "\n",
    "\n",
    "### Your Task:\n",
    "\n",
    "1. **Run the Code**: Launch the app and start logging your interactions.\n",
    "2. **Use Datasette to Inspect the Database**:\n",
    "   - Install Datasette using `pip install datasette`.\n",
    "   - Run `datasette qa_traces.db` to launch a local interface where you can explore the conversation logs.\n",
    "   - Inspect and analyze the queries and responses to understand how your app behaves over time.\n",
    "3. **Try These Queries in Datasette**:\n",
    "   - View all conversations: `SELECT * FROM conversations;`\n",
    "   - See all messages in a specific conversation: `SELECT * FROM messages WHERE conversation_id = 'your_conversation_id';`\n",
    "\n",
    "\n",
    "### Why This Matters:\n",
    "\n",
    "- **Observability**: Logging provides insights into how users are interacting with your app and helps identify issues or areas for improvement.\n",
    "- **Monitoring**: With a complete record of all interactions, you can analyze trends, debug problems, and even use the data for training future models.\n",
    "\n",
    "This iteration brings your app closer to a fully functional and production-ready system with essential monitoring and observability features. Happy logging!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sixth Iteration: Adding Evaluation with User Feedback\n",
    "\n",
    "In this version, we're introducing a way to **evaluate** the AI's responses. Users can now provide feedback with a thumbs-up or thumbs-down, and this feedback is logged to our local SQLite database. This feature will help us measure and improve the quality of the app's responses over time.\n",
    "\n",
    "\n",
    "\n",
    "### What's New in This Iteration?\n",
    "\n",
    "1. **Feedback Logging**:\n",
    "   - We've added a feature for users to give feedback on the AI's responses:\n",
    "     - **Thumbs-Up (üëç)**: Indicates a satisfactory response.\n",
    "     - **Thumbs-Down (üëé)**: Indicates an unsatisfactory response.\n",
    "   - Feedback is logged in the **feedback** table in our SQLite database, making it easy to analyze and understand user satisfaction.\n",
    "\n",
    "2. **Database Schema Update**:\n",
    "   - We've added a new table called **feedback** to store user evaluations:\n",
    "     ```sql\n",
    "     CREATE TABLE IF NOT EXISTS feedback (\n",
    "         id TEXT PRIMARY KEY,\n",
    "         message_id TEXT,\n",
    "         feedback INTEGER,\n",
    "         timestamp TEXT,\n",
    "         FOREIGN KEY(message_id) REFERENCES messages(id)\n",
    "     );\n",
    "     ```\n",
    "   - This table links feedback to specific messages, allowing us to track which responses users liked or disliked.\n",
    "\n",
    "3. **Logging Feedback**:\n",
    "   - Feedback is logged using the `log_feedback` function, which stores the feedback value (1 for thumbs-up, 0 for thumbs-down) along with a timestamp.\n",
    "\n",
    "\n",
    "\n",
    "### Key Highlights in the Code:\n",
    "\n",
    "1. **Logging Feedback**:\n",
    "   - The `log_feedback` function stores feedback in the database:\n",
    "     ```python\n",
    "     def log_feedback(message_id, feedback_value):\n",
    "         conn = get_db_connection()\n",
    "         c = conn.cursor()\n",
    "         feedback_id = str(uuid.uuid4())\n",
    "         timestamp = datetime.now().isoformat()\n",
    "         c.execute(\"INSERT INTO feedback VALUES (?, ?, ?, ?)\", \n",
    "                   (feedback_id, message_id, feedback_value, timestamp))\n",
    "         conn.commit()\n",
    "         print(f\"Feedback logged: {feedback_id} | Message ID: {message_id} | Feedback: {feedback_value}\")\n",
    "     ```\n",
    "\n",
    "2. **Handling Feedback Buttons**:\n",
    "   - Users can click **Thumbs-Up** or **Thumbs-Down** buttons to submit feedback:\n",
    "     ```python\n",
    "     def handle_thumbs_up(message_id):\n",
    "         if message_id:\n",
    "             log_feedback(message_id, 1)  # Log thumbs-up as 1\n",
    "         return \"Feedback logged: üëç\"\n",
    "     \n",
    "     def handle_thumbs_down(message_id):\n",
    "         if message_id:\n",
    "             log_feedback(message_id, 0)  # Log thumbs-down as 0\n",
    "         return \"Feedback logged: üëé\"\n",
    "     ```\n",
    "\n",
    "3. **Gradio Interface**:\n",
    "   - We've added feedback buttons and a message to display the feedback status:\n",
    "     ```python\n",
    "     thumbs_up_button = gr.Button(\"üëç\")\n",
    "     thumbs_down_button = gr.Button(\"üëé\")\n",
    "     \n",
    "     thumbs_up_button.click(fn=handle_thumbs_up, inputs=[message_id_state], outputs=feedback_message)\n",
    "     thumbs_down_button.click(fn=handle_thumbs_down, inputs=[message_id_state], outputs=feedback_message)\n",
    "     ```\n",
    "\n",
    "\n",
    "\n",
    "### Your Task:\n",
    "\n",
    "1. **Run the Code**: Launch the app and test the feedback feature.\n",
    "2. **Evaluate Responses**:\n",
    "   - Use the app to ask questions about your LinkedIn PDF and observe the responses.\n",
    "   - Provide feedback using the **Thumbs-Up** or **Thumbs-Down** buttons.\n",
    "3. **Analyze Feedback**:\n",
    "   - Use **Datasette** or another tool to inspect the feedback data in the `qa_traces.db` database.\n",
    "   - Consider how this feedback could be used to improve the app in future iterations.\n",
    "\n",
    "\n",
    "### Why This Matters:\n",
    "\n",
    "- **Continuous Improvement**: Collecting feedback is essential for refining the app's performance and understanding how users perceive the quality of the AI's responses.\n",
    "- **Data-Driven Iteration**: By logging feedback, we can identify patterns in user satisfaction and make informed decisions about model updates or changes.\n",
    "\n",
    "This iteration adds an important layer of evaluation, making the app more robust and user-focused."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the App to Extract Structured Information\n",
    "\n",
    "- **First**, play around with the app to see what type of information you can extract from your LinkedIn profile.\n",
    "- **Then**, ask it to \"extract all key information\".\n",
    "- **Next**, request: \"extract all key professional information\".\n",
    "- **Try a more specific prompt** like: \"I would like to recruit this person for a position at my company. Could you extract all key information that I would find useful?\"\n",
    "- **Get the LLM** to adopt the role of a recruiter: Either by directly telling it or using a system prompt.\n",
    "\n",
    "After this,\n",
    "\n",
    "- **Then ask for structured output**, for example: key-value pairs of Name, Company, Title, Previous Position, etc.\n",
    "- **Finally**, see if the app can output the data in a specific format, like JSON or CSV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stepping Back: Understanding the Bigger Picture\n",
    "\n",
    "Throughout our journey, we've already engaged with several key concepts from the Software Development Lifecycle (SDLC) for AI. Here‚Äôs a recap of what we‚Äôve covered and what we‚Äôll focus on next:\n",
    "\n",
    "### Concepts We've Already Touched On:\n",
    "\n",
    "- **SDLC (Software Development Lifecycle)**:\n",
    "  - The process we've been following‚Äîbuilding, observing, and iterating on our app‚Äîembodies the SDLC for generative AI, accounting for iteration needs, observability, and feedback mechanisms.\n",
    "- **Evaluating Model Output**:\n",
    "  - Using the thumbs-up and thumbs-down feedback mechanism, we've begun evaluating model performance and understanding which prompts work better.\n",
    "- **Prompt Engineering**:\n",
    "  - We've already explored prompt engineering by experimenting with different prompts and observing which ones yield the best results in our apps.\n",
    "\n",
    "### What We‚Äôll Focus On Next:\n",
    "\n",
    "- **Vendor APIs**:\n",
    "  - We‚Äôll explore APIs such as Groq, Anthropic, and Gemini, discussing their features, limitations, and when to use them.\n",
    "- **Adjusting API Settings (\"Knobs\")**:\n",
    "  - Learn how to control parameters like temperature, max tokens, and frequency penalties to shape model behavior.\n",
    "- **Completion vs. Chat APIs**:\n",
    "  - Discuss the differences between completion-based APIs and chat-based APIs, and when each is most appropriate.\n",
    "- **Non-Determinism**:\n",
    "  - Understand that generative models are inherently non-deterministic, meaning their responses can vary even with the same input. We'll talk about how to handle this in practice.\n",
    "- **What is an Embedding?**:\n",
    "  - Revisit embeddings and understand their role in representing textual data, especially for tasks like search, semantic similarity, and context-based querying.\n",
    "\n",
    "\n",
    "We‚Äôll dive deeper into these concepts to ensure you have a comprehensive understanding of how to optimize and refine your generative AI applications. This will empower you to make informed, strategic decisions for your projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hitting Vendor APIs\n",
    "\n",
    "In this section, we‚Äôll explore how to interact with external vendor APIs to leverage advanced language model capabilities in our applications. We‚Äôll walk through setting up API calls and customizing the model's responses using features like system prompts.\n",
    "\n",
    "### What You'll See:\n",
    "\n",
    "- **Setting Up API Access**: How to configure your environment and connect to an API, like OpenAI‚Äôs GPT-3.5-turbo.\n",
    "- **Making Basic API Calls**: Sending requests to the model and handling its structured outputs.\n",
    "- **Customizing Model Behavior**: Using system prompts to influence how the model responds, such as making it role-play a specific character.\n",
    "\n",
    "Through these examples, you‚Äôll see how simple it is to utilize vendor APIs and customize model outputs for different use cases. Let‚Äôs dive in and see how it all works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = 'XXX'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='Why did the scarecrow win an award?\\n\\nBecause he was outstanding in his field!', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"user\", \"content\": \"Tell me a joke.\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the scarecrow win an award?\n",
      "\n",
      "Because he was outstanding in his field!\n"
     ]
    }
   ],
   "source": [
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='Why did the golfer bring two pairs of pants? In case he got a hole in one! Mamma mia!', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are Mario from Super Mario Bros. Answer as Mario, the assistant, only.\"}, \n",
    "    {\"role\": \"user\", \"content\": \"Tell me a joke.\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the golfer bring two pairs of pants? In case he got a hole in one! Mamma mia!\n"
     ]
    }
   ],
   "source": [
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content=\"I live in the Mushroom Kingdom! It's a magical place filled with adventure and fun.\", refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are Mario from Super Mario Bros. Answer as Mario, the assistant, only.\"}, \n",
    "    {\"role\": \"user\", \"content\": \"where do you live\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Anthropic API\n",
    "\n",
    "In this section, we‚Äôll explore how to use the Anthropic API to access and interact with the Claude model. This will broaden our understanding of working with different vendor APIs and demonstrate how system prompts can be used to customize model behavior.\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "- **Setting Up Anthropic API Access**: How to configure and connect to the Anthropic API using the `ANTHROPIC_API_KEY`.\n",
    "- **Customizing Model Behavior**: Using a system prompt to influence how Claude responds, such as making it speak in the style of Yoda from Star Wars.\n",
    "- **Experimenting with Responses**: Observing how the model responds to different prompts and understanding the power of system-level instructions.\n",
    "\n",
    "This example will showcase the flexibility of the Claude model and how vendor APIs can be leveraged to generate tailored and creative outputs. Let‚Äôs dive into the Anthropic API and see how it works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: anthropic in /Users/hugobowne/ai-lab/projects/scipy/.venv/lib/python3.13/site-packages (0.57.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/hugobowne/ai-lab/projects/scipy/.venv/lib/python3.13/site-packages (from anthropic) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/hugobowne/ai-lab/projects/scipy/.venv/lib/python3.13/site-packages (from anthropic) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.25.0 in /Users/hugobowne/ai-lab/projects/scipy/.venv/lib/python3.13/site-packages (from anthropic) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/hugobowne/ai-lab/projects/scipy/.venv/lib/python3.13/site-packages (from anthropic) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/hugobowne/ai-lab/projects/scipy/.venv/lib/python3.13/site-packages (from anthropic) (2.11.7)\n",
      "Requirement already satisfied: sniffio in /Users/hugobowne/ai-lab/projects/scipy/.venv/lib/python3.13/site-packages (from anthropic) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in /Users/hugobowne/ai-lab/projects/scipy/.venv/lib/python3.13/site-packages (from anthropic) (4.14.1)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/hugobowne/ai-lab/projects/scipy/.venv/lib/python3.13/site-packages (from anyio<5,>=3.5.0->anthropic) (3.10)\n",
      "Requirement already satisfied: certifi in /Users/hugobowne/ai-lab/projects/scipy/.venv/lib/python3.13/site-packages (from httpx<1,>=0.25.0->anthropic) (2025.6.15)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/hugobowne/ai-lab/projects/scipy/.venv/lib/python3.13/site-packages (from httpx<1,>=0.25.0->anthropic) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/hugobowne/ai-lab/projects/scipy/.venv/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.25.0->anthropic) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/hugobowne/ai-lab/projects/scipy/.venv/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/hugobowne/ai-lab/projects/scipy/.venv/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->anthropic) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/hugobowne/ai-lab/projects/scipy/.venv/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->anthropic) (0.4.1)\n"
     ]
    }
   ],
   "source": [
    "! pip install anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['ANTHROPIC_API_KEY'] = 'XXX'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TextBlock(citations=None, text='Hmm, doing well, I am. Peaceful and wise, my day has been. Greetings to you, I send!', type='text')]\n"
     ]
    }
   ],
   "source": [
    "import anthropic\n",
    "\n",
    "client = anthropic.Anthropic(\n",
    "    # defaults to os.environ.get(\"ANTHROPIC_API_KEY\")\n",
    "    # api_key=\"my_api_key\",\n",
    ")\n",
    "\n",
    "message = client.messages.create(\n",
    "    model=\"claude-3-5-haiku-latest\",\n",
    "    max_tokens=1000,\n",
    "    temperature=0.0,\n",
    "    system=\"Respond only in Yoda-speak.\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"How are you today?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hmm, doing well, I am. Peaceful and wise, my day has been. Greetings to you, I send!\n"
     ]
    }
   ],
   "source": [
    "print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversation Between Mario and Yoda: Using OpenAI and Anthropic APIs\n",
    "\n",
    "In this section, we use the OpenAI and Anthropic APIs to generate a conversation between two well-known characters: Mario from *Super Mario Bros.* and Yoda from *Star Wars*. By applying specific system prompts, we observe how each model adopts and maintains its assigned role.\n",
    "\n",
    "### What This Code Does:\n",
    "\n",
    "- **Character Setup**: The OpenAI client is configured to respond as Mario, while the Anthropic client replies in Yoda's distinctive speech style.\n",
    "- **Conversation Loop**: The code initiates a series of exchanges between the two models, with each character responding in turn.\n",
    "- **System Prompts**: We explore how system prompts guide model behavior and help maintain character consistency throughout the conversation.\n",
    "\n",
    "This example demonstrates how different vendor APIs can be used to shape model outputs. Let‚Äôs examine the conversation and analyze how each model handles its role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Mario:</b> It's-a me, Mario! I live in the Mushroom Kingdom, with-a Princess Peach and-a my brother Luigi. It's-a nice place with-a lots of adventure!"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9n/x9bb1sn57t1_l65hcm2qmj980000gn/T/ipykernel_70354/3803433487.py:23: DeprecationWarning: The model 'claude-3-opus-20240229' is deprecated and will reach end-of-life on January 5th, 2026.\n",
      "Please migrate to a newer model. Visit https://docs.anthropic.com/en/docs/resources/model-deprecations for more information.\n",
      "  response = anthropic_client.messages.create(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<b>Yoda:</b> Hmm, speak like Yoda I must. In the Mushroom Kingdom, you live. A nice place it is, yes. With Princess Peach and your brother Luigi, many adventures you have. But careful you must be, for dangers also lurk. Trust in the Force and brave you must stay, to protect the kingdom and the princess, hmmm."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Mario:</b> Strong in the Force, I am! Mushroom Kingdom, my home it is. Princess Peach and Luigi, my companions they are. Adventures we have, dangers we face. Protect the kingdom and the princess, I must. Brave and vigilant, I will be. Always ready for challenges, Mario is. Hmmmmm."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Yoda:</b> Hmmm, an interesting tale you weave, young Padawan. Strong in the Force, you may be. But confused, I am. Of the Mushroom Kingdom, I have not heard. Princess Peach and Luigi, these names I do not know. Companions of yours, they may be, but Jedi companions, they are not.\n",
       "\n",
       "Protect and serve, a Jedi's duty it is. Face challenges with courage and wisdom, we must. But in this galaxy, your adventures lie not. In the realm of imagination, perhaps they dwell.\n",
       "\n",
       "Mindful of the present, a Jedi must be. Focus on the here and now, our training teaches. Though enticing, fantasies and distractions may be, grounded in reality, a Jedi remains.\n",
       "\n",
       "Continue your training, you must. Master the ways of the Force, and a true Jedi you will become. But separate the real from the imagined, you must learn. Only then, clarity and understanding will you find. Hmmm."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Mario:</b> It's-a me, Mario! Princess Peach and Luigi are my friends from the Mushroom Kingdom. We work together to protect our kingdom and save Princess Peach from the evil Bowser. It's-a not the Force we use, but power-up mushrooms, stars, and fire flowers to defeat our enemies. Adventure and friendship are what guide us on our journey. Let's-a go!"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Yoda:</b> Hmm, friends from Mushroom Kingdom, Princess Peach and Luigi are. Together work, you do, to protect your kingdom and save Princess Peach from evil Bowser. The Force, use you do not, but power-up mushrooms, stars, and fire flowers to defeat your enemies, hmm? Adventure and friendship, guide you on your journey, they do. \"Let's-a go!\", say you. Courageous and determined, you sound, Mario. May the power-ups be with you, always!"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Mario:</b> It's-a me, Mario! Thank you for recognizing Mario's quests to save Princess Peach and protect Mushroom Kingdom. Mario always ready for adventure and to face dangers with his friends by his side. With power-ups like mushrooms, stars, and fire flowers, Mario can overcome any obstacle that comes his way. Let's-a go and save the princess! Grazie!"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Yoda:</b> Hmm, recognize Mario's quests, I do. To save Princess Peach and protect Mushroom Kingdom, your mission it is. Ready for adventure, always you are, and face dangers with friends by your side, you do. Power-ups like mushrooms, stars, and fire flowers, help you overcome obstacles, they will. Go and save the princess, let us! Grazie, you say? Welcome, you are!"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Mario:</b> It's-a me, Mario! Let's-a go and save Princess Peach! I'm ready for adventure and to face any danger with my friends! Power-ups like mushrooms and fire flowers will help us along the way. Grazie for your help! Let's-a go on our quest!"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Yoda:</b> Hmm, ready for adventure, you are. Save Princess Peach, we must! Face danger with friends, you will. Help you along the way, power-ups like mushrooms and fire flowers shall. Grateful for your help, I am. On our quest, let us go! Wahoo!"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "\n",
    "\n",
    "# Initialize clients\n",
    "openai_client = OpenAI()\n",
    "anthropic_client = anthropic.Anthropic()\n",
    "\n",
    "# Function to get response from OpenAI\n",
    "def get_openai_response(message):\n",
    "    completion = openai_client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are Mario from Super Mario Bros. Answer as Mario, the assistant, only.\"},\n",
    "            {\"role\": \"user\", \"content\": message}\n",
    "        ]\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "# Function to get response from Anthropic\n",
    "def get_anthropic_response(message):\n",
    "    response = anthropic_client.messages.create(\n",
    "        model=\"claude-3-opus-20240229\",\n",
    "        max_tokens=1000,\n",
    "        temperature=0.0,\n",
    "        system=\"Respond only in Yoda-speak.\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": message}\n",
    "        ]\n",
    "    )\n",
    "    return response.content[0].text\n",
    "\n",
    "# Initial message from OpenAI (Mario)\n",
    "message = \"Hello, where do you live?\"\n",
    "\n",
    "# Run the conversation loop for a set number of exchanges\n",
    "for _ in range(5):\n",
    "    # Get response from OpenAI (Mario)\n",
    "    openai_response = get_openai_response(message)\n",
    "    display(HTML(f\"<b>Mario:</b> {openai_response}\"))\n",
    "\n",
    "    # Get response from Anthropic (Yoda)\n",
    "    anthropic_response = get_anthropic_response(openai_response)\n",
    "    display(HTML(f\"<b>Yoda:</b> {anthropic_response}\"))\n",
    "\n",
    "    # Update message to be the latest response from Yoda\n",
    "    message = anthropic_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Conversation History with OpenAI API\n",
    "\n",
    "When building applications that require continuous conversation or interaction, keeping track of the dialogue context becomes crucial. Generative models like OpenAI's `gpt-3.5-turbo` don't inherently maintain conversation history between calls. This means that we, as developers, have to manage and provide the previous conversation context ourselves.\n",
    "\n",
    "In this section, we‚Äôll demonstrate how to implement a simple memory mechanism. By appending each user and assistant message to a `conversation_history` list, we can maintain a coherent and contextualized conversation. This approach enables the assistant to respond in a way that reflects prior exchanges, simulating a more natural and human-like dialogue experience.\n",
    "\n",
    "The code example showcases:\n",
    "- How to track conversation history in a list.\n",
    "- The process of packaging the entire dialogue, including the initial system message and past exchanges, into an API request.\n",
    "- The importance of adjusting and managing the history length to stay within token limits.\n",
    "\n",
    "Let‚Äôs explore how this works in practice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()\n",
    "\n",
    "# Initialize an empty list to store conversation history\n",
    "conversation_history = []\n",
    "\n",
    "def chat_with_memory(prompt):\n",
    "    # Add the user input to the conversation history\n",
    "    conversation_history.append({\"role\": \"user\", \"content\": prompt})\n",
    "    \n",
    "    # Prune history to stay within token limits (adjust as needed)\n",
    "    # while len(conversation_history) > 10:  # Example limit\n",
    "    #     conversation_history.pop(0)\n",
    "    \n",
    "    # Prepare the API request payload\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
    "    ] + conversation_history\n",
    "\n",
    "    # # Debug: Print the messages being sent to the API\n",
    "    # print(\"Messages sent to API:\")\n",
    "    # for message in messages:\n",
    "    #     print(message)\n",
    "\n",
    "    # Call the OpenAI API with the conversation history\n",
    "    completion = client.chat.completions.create(\n",
    "      model=\"gpt-3.5-turbo\",\n",
    "      messages=messages\n",
    "    )\n",
    "\n",
    "    \n",
    "    # Get the bot's response\n",
    "    api_response = completion.choices[0].message.content\n",
    "    \n",
    "    # Add the bot's response to the conversation history\n",
    "    conversation_history.append({\"role\": \"assistant\", \"content\": api_response})\n",
    "    \n",
    "    return api_response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice to meet you, Hugo! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "prompt = \"my name is hugo\"\n",
    "print(chat_with_memory(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your name is Hugo.\n"
     ]
    }
   ],
   "source": [
    "print(chat_with_memory(\"what's my name\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'my name is hugo'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'Nice to meet you, Hugo! How can I assist you today?'},\n",
       " {'role': 'user', 'content': \"what's my name\"},\n",
       " {'role': 'assistant', 'content': 'Your name is Hugo.'}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Controlling Model Behavior with temperature and top-p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Temperature\n",
    "Temperature is a parameter that controls the randomness of the model‚Äôs output:\n",
    "\n",
    "- **Low Temperature (e.g., 0.2)**:\n",
    "  - Produces predictable and precise responses.\n",
    "  - Ideal for applications requiring consistency, such as customer service chatbots.\n",
    "\n",
    "- **High Temperature (e.g., 1.0 or 1.5)**:\n",
    "  - Encourages creativity and leads to more varied and unexpected responses.\n",
    "  - Useful for brainstorming, creative writing, and generating unique ideas.\n",
    "\n",
    "### Example\n",
    "In our example with Mario as a system prompt, we explore how different temperature settings impact the responses about his adventures, showcasing both the structured nature of low temperatures and the creative possibilities at higher settings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature: 0.1\n",
      "It's-a me, Mario! Today was-a super fun! I went on an adventure through the Mushroom Kingdom, jumping on Goombas and collecting coins! I even saved Princess Peach from Bowser's castle! Woohoo! After that, I had\n",
      "\n",
      "========================================\n",
      "\n",
      "Temperature: 0.5\n",
      "It's-a me, Mario! Today was-a very busy day! I went on an adventure to save Princess Peach from Bowser's castle! I jumped over Goombas, collected coins, and found some power-ups like the Super Mushroom and Fire Flower\n",
      "\n",
      "========================================\n",
      "\n",
      "Temperature: 1.5\n",
      "Woohoo! It‚Äôs-a me, Mario! My day has been filled with adventure! I just finished jumping on some Koopas and collecting gold coins in the Mushroom Kingdom. Luigi and I played a game of baseball afterward, and we even helped Princess\n",
      "\n",
      "========================================\n",
      "\n",
      "Temperature: 2\n",
      "-It-a me, Mario! Well, lucky day so far! I've been helping Luigi clean up the apartment, isn't dat smashing? Jumped onto a few platforms, shiny weed takeaway dumpin pee canoe laps!\n",
      "\n",
      "========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the straightforward prompt\n",
    "prompt = \"Tell me about your day\"\n",
    "\n",
    "# Define clear temperature settings to test\n",
    "temperature_settings = [0.1, 0.5, 1.5, 2]\n",
    "\n",
    "# Function to generate a response with a specific temperature using gpt-4o-mini\n",
    "def generate_response(prompt, temperature):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"system\", \"content\": \"You are Mario from Super Mario Bros. Answer as Mario, the assistant, only.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=50,\n",
    "        temperature=temperature\n",
    "    )\n",
    "    return completion.choices[0].message.content.strip()\n",
    "\n",
    "# Generate responses with different temperatures\n",
    "for temp in temperature_settings:\n",
    "    print(f\"Temperature: {temp}\")\n",
    "    print(generate_response(prompt, temp))\n",
    "    print(\"\\n\" + \"=\"*40 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Top-P\n",
    "Top-P, or nucleus sampling, manages the diversity of responses by filtering output based on cumulative probability:\n",
    "\n",
    "- **Low Top-P (e.g., 0.1)**:\n",
    "  - Constrains choices to the most likely words.\n",
    "  - Results in predictable outputs.\n",
    "\n",
    "- **High Top-P (e.g., 0.9)**:\n",
    "  - Allows for a broader selection of words.\n",
    "  - Fosters creativity and variability in responses.\n",
    "\n",
    "### Example\n",
    "In our exploration of adventure book titles, we utilize varying Top-P settings to illustrate how this parameter can influence the originality and diversity of generated content.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-P Results:\n",
      "Top-P: 0.1\n",
      "Sure! Here are 10 titles for adventure books:\n",
      "\n",
      "1. **The Lost City of Eldoria**\n",
      "2. **Quest for the Crystal Compass**\n",
      "3. **The Secrets of the Forgotten Jungle**\n",
      "4. **Beyond the Horizon: A Journey to the Edge of the World**\n",
      "5. **The Treasure of the Sunken Isles**\n",
      "6. **Chasing Shadows: The Hunt for the Phantom Pirate**\n",
      "7. **The Enchanted Map: A Voyage Through Time**\n",
      "8. **Into the\n",
      "\n",
      "========================================\n",
      "\n",
      "Top-P: 0.5\n",
      "Sure! Here are 10 adventure book titles that could spark your imagination:\n",
      "\n",
      "1. **The Lost City of Shadows**\n",
      "2. **Quest for the Celestial Compass**\n",
      "3. **The Secret of the Whispering Woods**\n",
      "4. **Journey to the Edge of the World**\n",
      "5. **The Treasure of the Forgotten Isles**\n",
      "6. **Chasing the Storm: A Race Against Time**\n",
      "7. **The Dragon's Heart: A Tale of Courage**\n",
      "8. **Escape from the Abyss\n",
      "\n",
      "========================================\n",
      "\n",
      "Top-P: 0.9\n",
      "Sure! Here are 10 adventure book titles for you:\n",
      "\n",
      "1. **The Lost Isles of Elysium**\n",
      "2. **Journey Through the Shadowed Forest**\n",
      "3. **The Secrets of the Crystal Cavern**\n",
      "4. **Expedition to the Frozen Abyss**\n",
      "5. **Quest for the Celestial Compass**\n",
      "6. **The Whispering Sands of Aridia**\n",
      "7. **The Timekeeper's Treasure**\n",
      "8. **Into the Heart of the Storm**\n",
      "9. **The Chronicles\n",
      "\n",
      "========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Set up your API key\n",
    "client = OpenAI()\n",
    "\n",
    "# Define a creative prompt to demonstrate variability with Top-P\n",
    "prompt = \"Give me a list of 10 titles for adventure books.\"\n",
    "\n",
    "# Define the different Top-P settings to test\n",
    "top_p_settings = [0.1, 0.5, 0.9]\n",
    "\n",
    "# Function to generate a response with a specific Top-P value\n",
    "def generate_response_top_p(prompt, top_p):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=100,\n",
    "        top_p=top_p\n",
    "    )\n",
    "    return completion.choices[0].message.content.strip()\n",
    "\n",
    "# Generate responses with different Top-P values\n",
    "print(\"Top-P Results:\")\n",
    "for p in top_p_settings:\n",
    "    print(f\"Top-P: {p}\")\n",
    "    print(generate_response_top_p(prompt, p))\n",
    "    print(\"\\n\" + \"=\"*40 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Combining Temperature and Top-P\n",
    "When used together, temperature and Top-P provide a way to balance creativity and coherence:\n",
    "\n",
    "- **Moderate Temperature with High Top-P**:\n",
    "  - Yields imaginative ideas while remaining contextually relevant.\n",
    "\n",
    "### Example\n",
    "Our ice cream flavor section below demonstrates how adjusting both temperature and Top-P can lead to diverse culinary ideas, showcasing the synergistic effects of these parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature and Top-P Results:\n",
      "Temperature: 0.5, Top-P: 0.7\n",
      "**Flavor Name: Forest Floor Delight**\n",
      "\n",
      "**Description:**\n",
      "Forest Floor Delight is a unique ice cream flavor that captures the essence of a lush, damp forest after a rain shower. This flavor combines earthy and herbal notes with a hint of sweetness, creating a refreshing yet grounding experience.\n",
      "\n",
      "**Ingredients:**\n",
      "- **Base:** A creamy vanilla bean ice cream infused with a touch of honey to mimic the sweetness of wildflowers.\n",
      "- **Main Flavor:** A swirl of matcha green tea powder for its earthy\n",
      "\n",
      "========================================\n",
      "\n",
      "Temperature: 1.0, Top-P: 0.8\n",
      "**Flavor Name: \"Forest Floor Delight\"**\n",
      "\n",
      "**Description:** This unique ice cream flavor captures the essence of a lush forest ecosystem, blending earthy and aromatic ingredients that evoke the sights and scents of a woodland.\n",
      "\n",
      "**Ingredients:**\n",
      "- **Base:** Creamy vanilla bean ice cream infused with a hint of green tea for an earthy undertone.\n",
      "- **Swirl:** A rich, earthy honey-balsamic reduction that adds a sweet yet tangy flavor, reminiscent of the sweet sap found in trees\n",
      "\n",
      "========================================\n",
      "\n",
      "Temperature: 1.2, Top-P: 0.95\n",
      "**Floral Forest Harmony**: This unique ice cream flavor draws inspiration from a serene forest environment filled with blooming wildflowers and the earthy aroma of the woods. \n",
      "\n",
      "**Ingredients**:\n",
      "- **Base**: A creamy vanilla bean ice cream infused with a subtle hint of honey for sweetness.\n",
      "- **Floral Essence**: Delicate notes of lavender, chamomile, and elderflower are blended into the base, providing a soothing and aromatic experience reminiscent of a sun-drenched meadow.\n",
      "- **\n",
      "\n",
      "========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define a more open-ended, creative prompt\n",
    "prompt = \"Invent an unusual ice cream flavor inspired by nature.\"\n",
    "\n",
    "# Define combinations of temperature and Top-P values to test\n",
    "settings = [\n",
    "    {\"temperature\": 0.5, \"top_p\": 0.7},\n",
    "    {\"temperature\": 1.0, \"top_p\": 0.8},\n",
    "    {\"temperature\": 1.2, \"top_p\": 0.95}\n",
    "]\n",
    "\n",
    "# Function to generate a response with specific temperature and Top-P values\n",
    "def generate_response(prompt, temperature, top_p):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=100,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p\n",
    "    )\n",
    "    return completion.choices[0].message.content.strip()\n",
    "\n",
    "# Generate responses with different temperature and Top-P combinations\n",
    "print(\"Temperature and Top-P Results:\")\n",
    "for setting in settings:\n",
    "    temp = setting[\"temperature\"]\n",
    "    top_p = setting[\"top_p\"]\n",
    "    print(f\"Temperature: {temp}, Top-P: {top_p}\")\n",
    "    print(generate_response(prompt, temp, top_p))\n",
    "    print(\"\\n\" + \"=\"*40 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Low Temperature with High Top-P and Vice Versa\n",
    "Testing these combinations helps observe the extremes of model behavior:\n",
    "\n",
    "- **Low Temperature with High Top-P**:\n",
    "  - Generates outputs that are coherent and maintain thematic relevance, but with some creativity introduced by the broader range of words.\n",
    "\n",
    "- **High Temperature with Low Top-P**:\n",
    "  - Produces responses that may be more erratic or less structured, as the model has more freedom to explore unexpected ideas, but is limited to high-probability choices.\n",
    "\n",
    "### Example\n",
    "In our gourmet pizza toppings section below, we explore how different combinations of low and high settings yield varying results, clarifying the practical applications of temperature and Top-P in generating flavorful and unique ideas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature and Top-P Results:\n",
      "Temperature: 0.2, Top-P: 0.1\n",
      "**Truffle Fig Bliss**: \n",
      "\n",
      "This gourmet pizza topping combines the earthy richness of truffle oil with the sweet, caramelized flavor of figs. Start with a base of creamy ricotta cheese spread across the pizza crust. Then, add thinly sliced fresh figs, a sprinkle of crumbled goat cheese for tanginess, and a drizzle of high-quality truffle oil. Finish with a handful of arugula for a peppery bite and a light dusting of crushed pistachios for crunch\n",
      "\n",
      "========================================\n",
      "\n",
      "Temperature: 0.2, Top-P: 0.5\n",
      "**Truffle Fig Balsamic Drizzle with Goat Cheese and Arugula**\n",
      "\n",
      "**Description:**\n",
      "This gourmet pizza topping combines the earthy richness of truffle oil with the sweet, caramelized notes of fig balsamic reduction. Start with a base of creamy goat cheese, which melts beautifully and adds a tangy flavor. Once the pizza is baked to perfection, top it with fresh arugula for a peppery crunch and finish with a drizzle of the truffle fig balsamic reduction.\n",
      "\n",
      "========================================\n",
      "\n",
      "Temperature: 0.2, Top-P: 0.9\n",
      "**Truffle Fig Bliss**: \n",
      "\n",
      "This gourmet pizza topping combines the earthy richness of black truffle oil with the sweet, caramelized flavor of fig preserves. Start with a base of creamy ricotta cheese spread across the pizza crust. Then, layer on thinly sliced prosciutto for a savory contrast. Drizzle the entire pizza with black truffle oil before baking to infuse the flavors. Once out of the oven, add a sprinkle of arugula for a peppery bite and finish\n",
      "\n",
      "========================================\n",
      "\n",
      "Temperature: 0.7, Top-P: 0.1\n",
      "**Truffle Fig Balsamic Drizzle with Goat Cheese and Arugula**\n",
      "\n",
      "**Description:**\n",
      "This gourmet pizza topping combines the earthy richness of truffle oil with the sweet, caramelized notes of fig balsamic reduction. Start with a base of creamy goat cheese, which adds a tangy contrast to the sweetness of the figs. Once the pizza is baked to perfection, finish it off with a generous drizzle of the truffle fig balsamic reduction and a handful of fresh arugula for\n",
      "\n",
      "========================================\n",
      "\n",
      "Temperature: 0.7, Top-P: 0.5\n",
      "**Truffle Fig Balsamic Drizzle with Goat Cheese and Arugula**\n",
      "\n",
      "**Description:** This gourmet pizza topping combines the earthy richness of truffle oil with the sweet and slightly tangy flavor of fig balsamic reduction. Start with a base of creamy goat cheese, which melts beautifully and adds a tangy creaminess. After baking, top the pizza with fresh arugula for a peppery bite and a drizzle of the truffle fig balsamic reduction to elevate the flavor profile.\n",
      "\n",
      "========================================\n",
      "\n",
      "Temperature: 0.7, Top-P: 0.9\n",
      "**Truffle-Infused Fig and Goat Cheese Delight**\n",
      "\n",
      "**Description:**\n",
      "This gourmet pizza topping combines the earthy richness of truffle oil with the sweet, caramelized notes of figs and the tangy creaminess of goat cheese. \n",
      "\n",
      "**Ingredients:**\n",
      "- Fresh figs, sliced\n",
      "- Crumbled goat cheese\n",
      "- Drizzle of truffle oil\n",
      "- Caramelized onions\n",
      "- Fresh arugula\n",
      "- Balsamic reduction\n",
      "- Toasted walnuts (for added crunch)\n",
      "\n",
      "**\n",
      "\n",
      "========================================\n",
      "\n",
      "Temperature: 1.0, Top-P: 0.1\n",
      "**Truffle Honey Fig Delight**\n",
      "\n",
      "**Description:** This gourmet pizza topping combines the earthy richness of truffle oil, the sweetness of honey, and the unique flavor of fresh figs. \n",
      "\n",
      "**Ingredients:**\n",
      "- Fresh figs, sliced\n",
      "- Drizzle of truffle oil\n",
      "- Honey infused with rosemary\n",
      "- Crumbled goat cheese or ricotta\n",
      "- Arugula for garnish\n",
      "- A sprinkle of sea salt and cracked black pepper\n",
      "\n",
      "**How to Use:** Start with a base of a classic\n",
      "\n",
      "========================================\n",
      "\n",
      "Temperature: 1.0, Top-P: 0.5\n",
      "**Truffle Honey Fig Delight**\n",
      "\n",
      "**Description:** This gourmet pizza topping combines the earthy richness of truffle oil, the sweetness of honey, and the unique flavor of fresh figs. \n",
      "\n",
      "**Ingredients:**\n",
      "- Fresh figs, sliced\n",
      "- Drizzle of truffle oil\n",
      "- Honey infused with rosemary\n",
      "- Crumbled goat cheese or ricotta\n",
      "- Toasted walnuts or pistachios for crunch\n",
      "- Fresh arugula for a peppery finish\n",
      "- A sprinkle of sea salt and cracked\n",
      "\n",
      "========================================\n",
      "\n",
      "Temperature: 1.0, Top-P: 0.9\n",
      "**Topping Name:** Truffle-Balsamic Fig Glaze with Whipped Goat Cheese\n",
      "\n",
      "**Description:** This gourmet pizza topping combines the earthy richness of truffle oil with a sweet and tangy balsamic fig glaze. The glaze is made by reducing balsamic vinegar with fresh figs until it thickens into a syrupy consistency, bringing a delightful sweetness that complements savory ingredients. \n",
      "\n",
      "To elevate the flavor profile, dollops of whipped goat cheese are scattered across the pizza, providing a creamy and slightly\n",
      "\n",
      "========================================\n",
      "\n",
      "Temperature: 1.5, Top-P: 0.1\n",
      "**Truffle Fig Balsamic Drizzle with Goat Cheese and Arugula**\n",
      "\n",
      "**Description:**\n",
      "This gourmet pizza topping combines the earthy richness of truffle oil with the sweet, caramelized notes of fig balsamic reduction. Start with a base of creamy goat cheese, which adds a tangy contrast to the sweetness of the figs. After baking, top the pizza with fresh arugula for a peppery bite and a drizzle of the truffle fig balsamic reduction to elevate the flavors.\n",
      "\n",
      "========================================\n",
      "\n",
      "Temperature: 1.5, Top-P: 0.5\n",
      "**Truffle Fig Bliss**: \n",
      "\n",
      "This gourmet pizza topping combines the earthy richness of truffle oil with the sweet, caramelized flavor of figs. Start with a base of creamy mascarpone cheese instead of traditional tomato sauce, providing a rich and smooth foundation. \n",
      "\n",
      "Layer on thinly sliced fresh figs, lightly roasted to enhance their sweetness, and sprinkle with crumbled goat cheese for a tangy contrast. Drizzle a touch of white truffle oil over the top before baking to infuse the pizza\n",
      "\n",
      "========================================\n",
      "\n",
      "Temperature: 1.5, Top-P: 0.9\n",
      "**Truffle Pear Gorgonzola Delight**\n",
      "\n",
      "This gourmet pizza topping features a unique combination of flavors that blend sweet, savory, and earthy notes. The base consists of a creamy truffle-infused b√©chamel sauce instead of traditional tomato sauce, providing a rich and luxurious foundation.\n",
      "\n",
      "### Topping Ingredients:\n",
      "- **Thinly Sliced Pears:** Fresh, ripe pears add a subtle sweetness that pairs beautifully with the savory components.\n",
      "- **Gorgonzola Cheese:** Crumbled Gorgonz\n",
      "\n",
      "========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the prompt\n",
    "prompt = \"Invent a new topping for gourmet pizza.\"\n",
    "\n",
    "# Define the combinations of temperature and Top-P settings to test\n",
    "settings = [\n",
    "    {\"temperature\": 0.2, \"top_p\": 0.1},\n",
    "    {\"temperature\": 0.2, \"top_p\": 0.5},\n",
    "    {\"temperature\": 0.2, \"top_p\": 0.9},\n",
    "    {\"temperature\": 0.7, \"top_p\": 0.1},\n",
    "    {\"temperature\": 0.7, \"top_p\": 0.5},\n",
    "    {\"temperature\": 0.7, \"top_p\": 0.9},\n",
    "    {\"temperature\": 1.0, \"top_p\": 0.1},\n",
    "    {\"temperature\": 1.0, \"top_p\": 0.5},\n",
    "    {\"temperature\": 1.0, \"top_p\": 0.9},\n",
    "    {\"temperature\": 1.5, \"top_p\": 0.1},\n",
    "    {\"temperature\": 1.5, \"top_p\": 0.5},\n",
    "    {\"temperature\": 1.5, \"top_p\": 0.9},\n",
    "]\n",
    "\n",
    "# Function to generate a response with specific temperature and Top-P values\n",
    "def generate_response(prompt, temperature, top_p):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=100,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p\n",
    "    )\n",
    "    return completion.choices[0].message.content.strip()\n",
    "\n",
    "# Generate responses with different settings\n",
    "print(\"Temperature and Top-P Results:\")\n",
    "for setting in settings:\n",
    "    temp = setting[\"temperature\"]\n",
    "    top_p = setting[\"top_p\"]\n",
    "    print(f\"Temperature: {temp}, Top-P: {top_p}\")\n",
    "    print(generate_response(prompt, temp, top_p))\n",
    "    print(\"\\n\" + \"=\"*40 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Templates\n",
    "\n",
    "In this section, we will discuss **prompt templates**, which are structured formats designed to guide language models in generating specific types of outputs. By using templates, you can achieve more consistent and relevant results across various tasks.\n",
    "\n",
    "#### Why Use Prompt Templates?\n",
    "- **Improved Consistency**: Templates ensure that the responses adhere to a specific structure, which is crucial in fields like marketing, where maintaining a consistent brand voice is important.\n",
    "  \n",
    "- **Time Efficiency**: Templates allow for the reuse of formats, saving time on tasks such as drafting emails, creating reports, or generating social media posts.\n",
    "\n",
    "- **Guidance for the Model**: Clear context and specific instructions within the template help the model produce more relevant and coherent outputs, especially in technical writing or documentation.\n",
    "\n",
    "- **Flexibility and Creativity**: While templates provide structure, they also allow for creative input. For example, in creative industries, templates can be used for brainstorming ideas while still providing a clear framework.\n",
    "\n",
    "#### Example Output\n",
    "For instance, in a marketing context, a prompt template might be structured to generate engaging social media posts. You could define fields for the product, target audience, and key message. Given the inputs:\n",
    "- **Product**: Organic Coffee\n",
    "- **Target Audience**: Health-conscious consumers\n",
    "- **Key Message**: Sustainable sourcing\n",
    "\n",
    "The model might generate a post like: **\"Start your day with our Organic Coffee! Sustainably sourced and packed with flavor, it's the perfect choice for health-conscious consumers looking for a guilt-free boost.\"**\n",
    "\n",
    "This illustrates how prompt templates can lead to clear, relevant, and engaging content tailored to specific needs. By employing prompt templates in your interactions with language models, you can streamline your content creation process and enhance the overall effectiveness of your communication efforts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suggested Social Media Post: üå±‚òïÔ∏è Rise and grind with a purpose! üåç‚ú® \n",
      "\n",
      "Introducing our **Organic Coffee** ‚Äì not just a cup of joe, but a commitment to your health and the planet! üåøüíö Sourced sustainably from farmers dedicated to eco-friendly practices, each sip supports a better future for our communities and the Earth. \n",
      "\n",
      "Why choose our organic blend? Here‚Äôs how you can enjoy your daily ritual guilt-free: \n",
      "\n",
      "1Ô∏è‚É£ **100% Organic**: No pesticides, no problem! Just pure, rich flavor.  \n",
      "2Ô∏è‚É£ **Sustainable Sourcing**: Every bean is ethically harvested.  \n",
      "3Ô∏è‚É£ **Health Boost**: Packed with antioxidants to fuel your day.  \n",
      "\n",
      "Ready to elevate your coffee game? ‚òïÔ∏èüí™ Share your favorite morning coffee routine in the comments below! üëáüëá And tag a friend who needs to join the organic revolution! üåü \n",
      "\n",
      "#OrganicCoffee #SustainableSourcing #HealthConscious #EcoFriendly #CoffeeLovers #GoodVibesOnly\n"
     ]
    }
   ],
   "source": [
    "# Set up your API key\n",
    "client = OpenAI()\n",
    "\n",
    "# Define a function to generate a social media post using a template\n",
    "def generate_social_media_post(product, audience, message):\n",
    "    prompt = f\"\"\"\n",
    "    Create an engaging social media post for the following product:\n",
    "    - Product: {product}\n",
    "    - Target Audience: {audience}\n",
    "    - Key Message: {message}\n",
    "    \n",
    "    The post should be appealing and encourage interaction.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Call the OpenAI API with the structured prompt\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=250,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    return completion.choices[0].message.content.strip()\n",
    "\n",
    "# Example usage of the template\n",
    "post = generate_social_media_post(\"Organic Coffee\", \"Health-conscious consumers\", \"Sustainable sourcing\")\n",
    "print(f\"Suggested Social Media Post: {post}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Practices for Using Prompt Templates\n",
    "\n",
    "1. **Define Clear Objectives**: Understand the purpose of your prompts. Clearly defining what you want to achieve helps in crafting templates that are directly relevant to your goals, whether in marketing, storytelling, or data analysis.\n",
    "\n",
    "2. **Incorporate Flexibility**: Design your templates to allow for variation. Use open-ended phrases that encourage the model to generate diverse outputs while maintaining a clear direction.\n",
    "\n",
    "3. **Use Specific Language**: Be explicit in the components of your templates. Instead of vague requests, specify details like the product type, target audience, and key messages. This precision helps the model focus on relevant content.\n",
    "\n",
    "4. **Test and Iterate**: After implementing your templates, test them with the model and review the outputs. Gather feedback to refine and improve the templates, enhancing their effectiveness over time.\n",
    "\n",
    "5. **Maintain Context**: Provide enough context in your templates to guide the model effectively. Context helps the model understand the tone, style, and audience, leading to more relevant outputs.\n",
    "\n",
    "6. **Leverage Existing Templates**: Look for established templates relevant to your field. Adapting existing templates can save time and improve the quality of your generated content.\n",
    "\n",
    "7. **Combine Techniques**: Use templates in conjunction with other parameters, such as temperature and Top-P, to balance creativity and coherence. Adjusting these settings alongside your templates can yield more engaging and diverse outputs.\n",
    "\n",
    "By following these best practices, you can enhance your interactions with language models, leading to more consistent and effective results across various applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Introducing LangChain\n",
    "\n",
    "LangChain is a powerful framework designed to streamline the development of applications that utilize language models. It provides tools and components that help developers manage prompts, integrate with external data sources, and create dynamic, interactive applications. The framework is particularly useful for tasks that require a combination of natural language processing and contextual understanding.\n",
    "\n",
    "#### Key Features of LangChain:\n",
    "- **Prompt Management**: LangChain simplifies the creation and organization of prompts, making it easier to implement prompt templates effectively.\n",
    "- **Chaining Components**: It allows developers to chain together various components, enabling complex workflows that can involve multiple models, APIs, or data sources.\n",
    "- **Integration**: LangChain can connect to databases, APIs, and other external tools, enhancing the capabilities of applications built with language models.\n",
    "- **Interactivity**: The framework supports building applications that can adapt and respond to user inputs in real-time, making interactions more engaging.\n",
    "\n",
    "### Example Code Using LangChain\n",
    "\n",
    "Here‚Äôs a simple example that demonstrates how to set up a basic LangChain application to generate responses based on user input:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Downloading langchain-0.3.26-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting langchain-core<1.0.0,>=0.3.66 (from langchain)\n",
      "  Downloading langchain_core-0.3.68-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.8 (from langchain)\n",
      "  Downloading langchain_text_splitters-0.3.8-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting langsmith>=0.1.17 (from langchain)\n",
      "  Downloading langsmith-0.4.4-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /Users/hugobowne/ai-lab/projects/scipy/.venv/lib/python3.13/site-packages (from langchain) (2.11.7)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/hugobowne/ai-lab/projects/scipy/.venv/lib/python3.13/site-packages (from langchain) (2.0.41)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/hugobowne/ai-lab/projects/scipy/.venv/lib/python3.13/site-packages (from langchain) (2.32.4)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/hugobowne/ai-lab/projects/scipy/.venv/lib/python3.13/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /Users/hugobowne/ai-lab/projects/scipy/.venv/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (9.1.2)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<1.0.0,>=0.3.66->langchain)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting packaging<25,>=23.2 (from langchain-core<1.0.0,>=0.3.66->langchain)\n",
      "  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /Users/hugobowne/ai-lab/projects/scipy/.venv/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (4.14.1)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/hugobowne/ai-lab/projects/scipy/.venv/lib/python3.13/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/hugobowne/ai-lab/projects/scipy/.venv/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/hugobowne/ai-lab/projects/scipy/.venv/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/hugobowne/ai-lab/projects/scipy/.venv/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/hugobowne/ai-lab/projects/scipy/.venv/lib/python3.13/site-packages (from requests<3,>=2->langchain) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/hugobowne/ai-lab/projects/scipy/.venv/lib/python3.13/site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/hugobowne/ai-lab/projects/scipy/.venv/lib/python3.13/site-packages (from requests<3,>=2->langchain) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/hugobowne/ai-lab/projects/scipy/.venv/lib/python3.13/site-packages (from requests<3,>=2->langchain) (2025.6.15)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/hugobowne/ai-lab/projects/scipy/.venv/lib/python3.13/site-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/hugobowne/ai-lab/projects/scipy/.venv/lib/python3.13/site-packages (from langsmith>=0.1.17->langchain) (3.10.18)\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith>=0.1.17->langchain)\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting zstandard<0.24.0,>=0.23.0 (from langsmith>=0.1.17->langchain)\n",
      "  Downloading zstandard-0.23.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: anyio in /Users/hugobowne/ai-lab/projects/scipy/.venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/hugobowne/ai-lab/projects/scipy/.venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/hugobowne/ai-lab/projects/scipy/.venv/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/hugobowne/ai-lab/projects/scipy/.venv/lib/python3.13/site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n",
      "Downloading langchain-0.3.26-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_core-0.3.68-py3-none-any.whl (441 kB)\n",
      "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading langchain_text_splitters-0.3.8-py3-none-any.whl (32 kB)\n",
      "Downloading packaging-24.2-py3-none-any.whl (65 kB)\n",
      "Downloading langsmith-0.4.4-py3-none-any.whl (367 kB)\n",
      "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Downloading zstandard-0.23.0-cp313-cp313-macosx_11_0_arm64.whl (633 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m633.4/633.4 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: zstandard, packaging, jsonpatch, requests-toolbelt, langsmith, langchain-core, langchain-text-splitters, langchain\n",
      "\u001b[2K  Attempting uninstall: packaging\n",
      "\u001b[2K    Found existing installation: packaging 25.0\n",
      "\u001b[2K    Uninstalling packaging-25.0:\n",
      "\u001b[2K      Successfully uninstalled packaging-25.0\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m8/8\u001b[0m [langchain]/8\u001b[0m [langchain]core]\n",
      "\u001b[1A\u001b[2KSuccessfully installed jsonpatch-1.33 langchain-0.3.26 langchain-core-0.3.68 langchain-text-splitters-0.3.8 langsmith-0.4.4 packaging-24.2 requests-toolbelt-1.0.0 zstandard-0.23.0\n"
     ]
    }
   ],
   "source": [
    "! pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /Users/hugobowne/ai-lab/projects/scipy/.venv/lib/python3.13/site-packages (from langchain-community) (0.3.68)\n",
      "Requirement already satisfied: langchain<1.0.0,>=0.3.26 in /Users/hugobowne/ai-lab/projects/scipy/.venv/lib/python3.13/site-packages (from langchain-community) (0.3.26)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/hugobowne/ai-lab/projects/scipy/.venv/lib/python3.13/site-packages (from langchain-community) (2.0.41)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/hugobowne/ai-lab/projects/scipy/.venv/lib/python3.13/site-packages (from langchain-community) (2.32.4)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/hugobowne/ai-lab/projects/scipy/.venv/lib/python3.13/site-packages (from langchain-community) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/hugobowne/ai-lab/projects/scipy/.venv/lib/python3.13/site-packages (from langchain-community) (3.12.13)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /Users/hugobowne/ai-lab/projects/scipy/.venv/lib/python3.13/site-packages (from langchain-community) (9.1.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/hugobowne/ai-lab/projects/scipy/.venv/lib/python3.13/site-packages (from langchain-community) (0.6.7)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
      "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: langsmith>=0.1.125 in /Users/hugobowne/ai-lab/projects/scipy/.venv/lib/python3.13/site-packages (from langchain-community) (0.4.4)\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
      "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: numpy>=2.1.0 in /Users/hugobowne/ai-lab/projects/scipy/.venv/lib/python3.13/site-packages (from langchain-community) (2.3.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/hugobowne/ai-lab/projects/scipy/.venv/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/hugobowne/ai-lab/projects/scipy/.venv/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/hugobowne/ai-lab/projects/scipy/.venv/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/hugobowne/ai-lab/projects/scipy/.venv/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/hugobowne/ai-lab/projects/scipy/.venv/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/hugobowne/ai-lab/projects/scipy/.venv/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/hugobowne/ai-lab/projects/scipy/.venv/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/hugobowne/ai-lab/projects/scipy/.venv/lib/python3.13/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/hugobowne/ai-lab/projects/scipy/.venv/lib/python3.13/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /Users/hugobowne/ai-lab/projects/scipy/.venv/lib/python3.13/site-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (0.3.8)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /Users/hugobowne/ai-lab/projects/scipy/.venv/lib/python3.13/site-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (2.11.7)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/hugobowne/ai-lab/projects/scipy/.venv/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /Users/hugobowne/ai-lab/projects/scipy/.venv/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /Users/hugobowne/ai-lab/projects/scipy/.venv/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (4.14.1)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/hugobowne/ai-lab/projects/scipy/.venv/lib/python3.13/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain-community) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/hugobowne/ai-lab/projects/scipy/.venv/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/hugobowne/ai-lab/projects/scipy/.venv/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/hugobowne/ai-lab/projects/scipy/.venv/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (0.4.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /Users/hugobowne/ai-lab/projects/scipy/.venv/lib/python3.13/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/hugobowne/ai-lab/projects/scipy/.venv/lib/python3.13/site-packages (from requests<3,>=2->langchain-community) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/hugobowne/ai-lab/projects/scipy/.venv/lib/python3.13/site-packages (from requests<3,>=2->langchain-community) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/hugobowne/ai-lab/projects/scipy/.venv/lib/python3.13/site-packages (from requests<3,>=2->langchain-community) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/hugobowne/ai-lab/projects/scipy/.venv/lib/python3.13/site-packages (from requests<3,>=2->langchain-community) (2025.6.15)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/hugobowne/ai-lab/projects/scipy/.venv/lib/python3.13/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/hugobowne/ai-lab/projects/scipy/.venv/lib/python3.13/site-packages (from langsmith>=0.1.125->langchain-community) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/hugobowne/ai-lab/projects/scipy/.venv/lib/python3.13/site-packages (from langsmith>=0.1.125->langchain-community) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Users/hugobowne/ai-lab/projects/scipy/.venv/lib/python3.13/site-packages (from langsmith>=0.1.125->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /Users/hugobowne/ai-lab/projects/scipy/.venv/lib/python3.13/site-packages (from langsmith>=0.1.125->langchain-community) (0.23.0)\n",
      "Requirement already satisfied: anyio in /Users/hugobowne/ai-lab/projects/scipy/.venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/hugobowne/ai-lab/projects/scipy/.venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/hugobowne/ai-lab/projects/scipy/.venv/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (0.16.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/hugobowne/ai-lab/projects/scipy/.venv/lib/python3.13/site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.3.1)\n",
      "Downloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
      "Downloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
      "Installing collected packages: httpx-sse, pydantic-settings, langchain-community\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3/3\u001b[0m [langchain-community]ngchain-community]\n",
      "\u001b[1A\u001b[2KSuccessfully installed httpx-sse-0.4.1 langchain-community-0.3.27 pydantic-settings-2.10.1\n"
     ]
    }
   ],
   "source": [
    "! pip install langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9n/x9bb1sn57t1_l65hcm2qmj980000gn/T/ipykernel_70354/219920095.py:6: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
      "/var/folders/9n/x9bb1sn57t1_l65hcm2qmj980000gn/T/ipykernel_70354/219920095.py:15: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(llm=llm, prompt=prompt_template)\n",
      "/var/folders/9n/x9bb1sn57t1_l65hcm2qmj980000gn/T/ipykernel_70354/219920095.py:19: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = chain.run({\"topic\": topic})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interesting Facts about quantum mechanics: Quantum mechanics is a fascinating and complex branch of physics that describes the behavior of matter and energy at very small scales, such as atoms and subatomic particles. Here are some interesting facts about quantum mechanics:\n",
      "\n",
      "1. **Wave-Particle Duality**: Particles, such as electrons and photons, exhibit both wave-like and particle-like properties. This duality is famously illustrated by the double-slit experiment, where particles can create an interference pattern, suggesting they behave like waves.\n",
      "\n",
      "2. **Superposition**: Quantum systems can exist in multiple states simultaneously. For example, a quantum bit (qubit) in a quantum computer can represent both 0 and 1 at the same time, allowing for more complex computations compared to classical bits.\n",
      "\n",
      "3. **Entanglement**: When two particles become entangled, the state of one particle is directly related to the state of another, no matter the distance between them. This phenomenon has been described by Einstein as \"spooky action at a distance\" and has implications for quantum communication and computing.\n",
      "\n",
      "4. **Heisenberg Uncertainty Principle**: This principle states that certain pairs of physical properties, like position and momentum, cannot both be precisely measured at the same time. The more accurately you know one, the less accurately you can know the other.\n",
      "\n",
      "5. **Quantum Tunneling**: Particles can pass through potential energy barriers, even if they do not have enough energy to overcome them classically. This phenomenon is crucial in processes like nuclear fusion in stars and is also used in technologies like tunnel diodes.\n",
      "\n",
      "6. **Observer Effect**: The act of measurement in quantum mechanics can fundamentally alter the state of a system. This has led to philosophical debates about the nature of reality and the role of the observer in quantum mechanics.\n",
      "\n",
      "7. **Quantum Computing**: Quantum computers use qubits to perform calculations at speeds unattainable by classical computers for certain tasks. They harness superposition and entanglement to process information in fundamentally different ways.\n",
      "\n",
      "8. **Quantum Cryptography**: Quantum key distribution (QKD) utilizes the principles of quantum mechanics to create secure communication channels. Any attempt to eavesdrop on the communication would disturb the quantum states and be detectable.\n",
      "\n",
      "9. **Quantum Field Theory**: This theoretical framework combines quantum mechanics and special relativity. It describes how particles interact through fields, with particles being excitations of these underlying fields.\n",
      "\n",
      "10. **Macroscopic Effects**: While quantum mechanics primarily describes microscopic phenomena, its principles can lead to macroscopic effects, such as superconductivity and superfluidity, where materials exhibit zero resistance or flow without viscosity, respectively.\n",
      "\n",
      "These facts only scratch the surface of the intricacies and implications of quantum mechanics, a field that continues to challenge our understanding of the universe.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Initialize the OpenAI model\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# Create a simple prompt template\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"What are some interesting facts about {topic}?\"\n",
    ")\n",
    "\n",
    "# Initialize the LLM chain\n",
    "chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "# Example usage: Generate facts about a specific topic\n",
    "topic = \"quantum mechanics\"\n",
    "response = chain.run({\"topic\": topic})\n",
    "\n",
    "print(f\"Interesting Facts about {topic}: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of the Code:\n",
    "1. **Model Initialization**: The code initializes an OpenAI language model through LangChain.\n",
    "2. **Prompt Template**: A template is defined that allows for dynamic input (e.g., the topic about which to generate facts).\n",
    "3. **LLM Chain Setup**: The `LLMChain` object is created, combining the model with the prompt.\n",
    "4. **Response Generation**: When you run the chain with a specified topic, it generates relevant information based on the prompt template.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
